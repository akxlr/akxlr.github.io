---
layout: post
title:  "Chapter 1"
main-title: "Introduction"
### Bibliography ###
lafferty2001conditional: "Lafferty et al. 2001"
liu2006conditional: "Liu 2006"
gog13: "Gogate and Domingos 2013"

jordan2003introduction: "Jordan 2003"
koller09: "Koller and Friedman 2009"
len20: "Lenz 1920"
isi25: "Ising 1925"
bis06: "Bishop 2006"
dom14: "Domke 2014"
pit36: "Pitman 1936"
besag1974spatial: "Besag 1974"
### end of Bibliography ###
---

## 1.1 The inference problem {#sec:motivation}

The process of *inference* -- the ability to take available information
and use it to derive logical consequences -- is central across a broad
range of fields including the sciences, engineering, medicine and law.
In machine learning and statistics, we are typically concerned with
*probabilistic inference*: given some model of the world and a set of
observations, what can we rationally deduce about the state of the
world? In more concrete terms, if we model facts about the world with
random variables, what is the probability that a particular random
variable or set of random variables will take certain values? For
example, in the setting of medical diagnosis we may use random variables
to represent symptoms, test results and the presence of various diseases
in a sick patient (Figure [1.1]({{ post_url | prepend: "#fig-1.1" }})).
If we have a model of the probability with which
certain diseases cause certain symptoms and positive tests, we can use
probabilistic inference to arrive at a rational belief about the
probability of the patient having various diseases based on observed
symptoms and test results. Other applications of probabilistic inference
are considerably more complex; examples include computer vision, natural
language processing, robot localisation and protein structure prediction
[{{page.koller09}}; {{page.lafferty2001conditional}}; {{page.liu2006conditional}}].

Graphical models are a generic framework for probabilistic modeling.
They allow compact representations of probabilistic beliefs in the form
of graphs, and provide a rich set of tools to perform inference
efficiently. Key to their utility is an inbuilt separation between
knowledge and reasoning: we can study inference algorithms over graphs
separately to any particular inference task, and these algorithms may be
used on any task once the domain knowledge particular to the task is
encoded into a graph structure appropriately [{{page.koller09}}].

In general, probabilistic inference is computationally hard. As the
number of variables and the complexity of their interactions increases,
it is difficult or impossible to perform most inference tasks exactly in
reasonable time. Finding efficient algorithms for probabilistic
inference over large graphical models is an active field of research,
and is the major topic of this thesis. Typically, we must resort to
approximate inference algorithms that have varying guarantees of
accuracy, memory usage and running time.

![fig1.1](/assets/images/fig_1.1.jpg)
{:.img.fig.fig_1_1 style="width: 45%"}

Quick Medical Reference (QMR) model. The $\{d_i\}$ represent diseases and $\{f_i\}$ represent findings including test results and symptoms. By encoding our beliefs about how diseases relate to findings, we can later infer the probability that a new patient has each disease based on observed findings (indicated by shaded nodes $f_1, f_4$).
{:.figure text='1.1' id='fig-1.1' style="text-align: justify;"}

## Contribution

The major contribution of this thesis is an original algorithm for
approximate inference in discrete graphical models, which we call
*low-rank tensor propagation*. The algorithm we describe is closely
related to the junction tree algorithm [{{page.koller09}}] and fits into the
framework of structured message passing proposed by [{{page.gog13}}]. We first
apply a tensor rank decomposition (defined in Section 5.1) to each
factor in the graphical model, decomposing the factor into a sum of
rank-one tensors. We then run a modified version of junction tree where
we represent all messages and intermediate factors in this same
decomposed form. Under these assumptions, our key observation is that
the two time-consuming steps of junction tree, marginalisation and
multiplication of functions, can be performed efficiently or
approximated. In particular, exact marginalisation is always cheap and
multiplication can be approximated by sampling rank-one terms from the
decompositions. This leads to an approximate message passing algorithm
that provides marginal estimates that converge asymptotically to the
true marginals. Our main theoretical result is that to guarantee
accurate results, the running time of the algorithm depends on certain
properties of the tensor decompositions rather than the treewidth of the
graph. Experimentally, we show that on Ising models and small random
graphs, the algorithm performs very well.

## Thesis structure

This thesis is structured as follows.

-   [Chapter 2]({% link _posts/2022-02-21-chapter-2.md %}) introduces the different types of
    probabilistic graphical models including Bayesian networks, Markov
    random fields and Factor graphs.

-   Chapter 3 explains how one can exploit the structure in
    these representations to perform inference efficiently, with exact
    results.

-   Chapter 4 gives a brief overview of common
    existing algorithms for approximate inference, used when the exact
    inference algorithms of Chapter
    3 are infeasible.

-   Chapter 5 describes our main contribution. We explain the
    low-rank tensor propagation algorithm in detail, present our
    theoretical analysis, and discuss the relationship to existing work.

-   Chapter 6 contains our experimental results,
    comparing the algorithm to existing methods.

-   Finally, in Chapter 7 we provide a brief review and suggest
    directions for future work.

## Notation

An index of notation is available as Appendix A. In addition, we use the
following conventions.

-   We generally omit subscripts from probability distributions when the
    meaning is clear from the context. For example, we write $p(x)p(y)$
    to mean $p_X(x)p_Y(y)$.

-   Superscripts are often used to represent indices rather than
    exponents.
