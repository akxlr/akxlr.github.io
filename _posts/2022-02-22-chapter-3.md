---
layout: post
title:  "Chapter 3"
main-title: "Exact inference"
### Bibliography ###
lafferty2001conditional: "Lafferty et al. 2001"
liu2006conditional: "Liu 2006"
gog13: "Gogate and Domingos 2013"

jordan2003introduction: "Jordan 2003"
koller09: "Koller and Friedman 2009"
len20: "Lenz 1920"
isi25: "Ising 1925"
bis06: "Bishop 2006"
dom14: "Domke 2014"
pit36: "Pitman 1936"
besag1974spatial: "Besag 1974"
### end of Bibliography ###
---

In this chapter we describe how the structure encoded by the PGMs
defined in Chapter [Chapter 2]({% link _posts/2022-02-21-chapter-2.md %})
can be used to efficiently perform exact
inference. *Exact* refers to the fact that the algorithms return the
same answers as if we had used brute-force, in contrast to the
approximate algorithms described in chapters
[Chapter 4]({% link _posts/2022-02-24-chapter-4.md %}) and
[Chapter 5]({% link _posts/2022-02-25-chapter-5.md %}). We
describe three related algorithms: the *elimination algorithm*, *belief
propagation* and *junction tree*. Ultimately, the tractability of exact
inference depends on a property of the graph called its *treewidth*,
which we define in [Section 3.3.1.1](#sec:triangulation).

## 3.1 Elimination algorithm {#sec:jt_structure}

The *elimination algorithm* (
[Algorithm 1](#alg:elimination)) operates on factor graphs and is the
simplest way to exploit the factorisation structure of a joint
distribution to obtain the marginal distribution of a single variable.
It makes use of the fact that sums and products can be reordered so that
sums need only be computed over a small state space. For example,
suppose

$$p(a,b,c,d)\propto \phi_{AB}(a,b)\phi_{AD}(a,d)\phi_{BC}(b,c)\phi_C(c)$$

where $a\in\{1,\ldots ,N_a\}$ and similarly for $b$, $c$ and $d$. To
find the marginal distribution of $A$, we can avoid the exponential
complexity described in [Definition 2.1.1]( {% link _posts/2022-02-21-chapter-2.md %}#def-2.1.1 ) by
reordering sums and products: 

$$
\begin{aligned}
    p(a) &\propto \sum_b \sum_c \sum_d \phi_{AB}(a,b)\phi_{AD}(a,d)\phi_{BC}(b,c)\phi_C(c) \\
    &= \sum_b \phi_{AB}(a,b) \sum_c \phi_{BC}(b,c)\phi_C(c) \sum_d \phi_{AD}(a,d).
\end{aligned}
$$

<!-- Ref for Algorithm 1 (blank html char)-->
&zwnj;
{:#alg:elimination style="height: 0;"}
<!-- end of Ref for Algorithm 1 -->

![alg1](/assets/images/alg_1.jpg)
{:.img.fig.alg_1 style="width: 100%"}

<br/>
The elimination algorithm utilises this reordering and proceeds to
eliminate variables one by one, in this case in the order $(D,C,B)$:

$$
\begin{aligned}
    p(a) &\propto \sum_b \phi_{AB}(a,b) \sum_c \phi_{BC}(b,c)\phi_C(c) \sum_d \phi_{AD}(a,d) \\
    &= \psi_1(a) \sum_b \phi_{AB}(a,b) \sum_c \phi_{BC}(b,c)\phi_C(c) & \psi_1(a) = \sum_d \phi_{AD}(a,d) \\
    &= \psi_1(a) \sum_b \phi_{AB}(a,b) \psi_2(b) & \psi_2(b) = \sum_c \phi_{BC}(b,c)\phi_C(c) \\
    &= \psi_1(a) \psi_3(a) & \psi_3(a) = \sum_b \phi_{AB}(a,b) \psi_2(b)
\end{aligned}
$$

and after performing this final multiplication and normalising, we get
$p(a)$ as required. The key point is that na√Øvely, computing $p(a)$
would involve a sum over a state space of size
$N_b \times N_c \times N_d$, exponential in the number of variables, but
by exploiting the factorisation we only need to compute the local sums
for $\psi_1$, $\psi_2$ and $\psi_3$ and thus substantially reduce this
cost. Also, notice that we do not need to do any extra work to
explicitly compute the normalisation constant $Z$ -- the normalisation
can be computed quickly at the end[^1].

### 3.1.1 Elimination order

An important issue in the elimination algorithm is how to choose the
order in which to eliminate variables (line 2 in [Algorithm 1](#alg:elimination)).
The speedup we are able to achieve 
depends directly on this choice, because it determines the size of the
intermediate *elimination cliques* $\psi'$ that are constructed as we
eliminate each variable (line
[\[line:elim_clique\]](#line:elim_clique){reference-type="ref"
reference="line:elim_clique"}). Computing $\sum_{x_j} \psi'$ takes
number of steps proportional to the size of the potential table for
$\psi'$, equal to the product of the sizes of the variables in its
scope. We therefore seek an elimination order that results in small
elimination cliques. Finding an optimal order is $\mathcal{NP}$-hard,
but there are effective heuristics we can use. For a detailed discussion
of these, see [@koller09].

## 3.2 Belief propagation on trees {#sec:bp}

The elimination algorithm demonstrates the key concept behind the
efficiency gains offered by PGMs -- reordering sums and products. The
following two algorithms, *belief propagation* and *junction tree*, use
the same fundamental concept but use a form of dynamic programming to
efficiently perform multiple queries at once. Belief propagation is
applicable only to tree-structured graphs,[^2] and junction tree extends
the same principle to general graphs. It is more convenient to reason
about these algorithms via MRFs, so for the following we assume without
loss of generality that our input graph is an MRF.

The belief propagation or *sum-product* algorithm is defined recursively
in terms of *messages* passed between neighbouring variables. A message
is always a function of the receiving variable, and represents an
intermediate marginalisation over a partial set of factors. The message
from each node $X_t$ to a neighbour $X_s$ is defined recursively as
$$\begin{aligned}
    \m{t}{s}(x_s) = \sum_{x_t} \phi_{t,s}(x_t,x_s) \phi_t(x_t) \prod_{u\neq s} \m{u}{t}(x_t).\label{eq:bp_msg}\end{aligned}$$
A node $X_t$ can send a message to its neighbour $X_s$ only when $X_t$
has received messages from all neighbours except $X_s$. Then, the
message it sends is the product of messages it has received from its
other neighbours multiplied with any local and pairwise potentials, and
marginalised to be a function of the receiving variable $X_s$. Since the
MRF is tree-structured by assumption, this recursion is well-defined.

::: algorithm
::: algorithmic
$X_i$.messages $\gets \varnothing$ Initialise incoming message buffers
$X_r \gets \Call{Choose\_root}{G}$ Send messages upstream Send messages
downstream **return** $\phi_i(x_i) \prod_{u} \m{u}{i}(x_i)$ for each
$X_i$, normalising if necessary

For each child $X_t$ of $X_s$ Recursively collect messages from children
of $X_t$
$\m{t}{s}(x_s) \gets \sum_{x_t} \phi_{t,s}(x_t,x_s) \phi_t(x_t) \prod_{u\neq s} \m{u}{t}(x_t)$
[]{#line:collect_msg label="line:collect_msg"} Store $\m{t}{s}$ in
$X_s$.messages Send message from $X_t$ to $X_s$ []{#line:collect_store
label="line:collect_store"}

For each child $X_t$ of $X_s$
$\m{s}{t}(x_t) \gets \sum_{x_s} \phi_{t,s}(x_t,x_s) \phi_t(x_s) \prod_{u \neq t} \m{u}{s}(x_s)$
[]{#line:distribute_msg label="line:distribute_msg"} Store $\m{s}{t}$ in
$X_t$.messages Send message from $X_s$ to $X_t$
[]{#line:distribute_store label="line:distribute_store"} Recursively
send messages to children of $X_t$
:::
:::

Belief propagation is described more concretely in Algorithm
$\ref{alg:bp}$. The root node returned by [Choose_root]{.smallcaps} can
be any variable. If we imagine the graph as a tree with root $X_r$ at
the top, [Collect($X_r$)]{.smallcaps} causes messages to be sent in an
upstream direction from the leaves to the root, and
[Distribute($X_r$)]{.smallcaps} sends messages downstream from the root
to the leaves. After calling both [Collect($X_r$)]{.smallcaps} and
[Distribute($X_r$)]{.smallcaps}, every node will have received a
downstream message from its parent and an upstream message from each of
its children. The marginal for each node $X_s$ is then given by
multiplying all incoming messages to $X_s$ with any local potential and
normalising. An inductive argument shows that this procedure returns the
desired marginals [@bis06; @koller09].

Belief propagation is performing essentially the same computations as
the elimination algorithm, but with the insight that many of the
intermediate marginalisation results computed in the elimination
algorithm are shared when computing marginals for different variables.
We are therefore able to compute the marginal distribution for every
variable with only roughly twice as many steps as for a single variable,
on tree structured distributions. In general, belief propagation as
described here can be applied to any tree-structured factor graph (or
equivalent Bayesian network or MRF).

### 3.2.1 Implementation

In the discrete case, functions are typically represented as
multidimensional floating point arrays and messages as floating point
vectors. Intermediate marginalisation and multiplication steps are
generally achieved by brute force iteration. Algorithm
[\[alg:bp\]](#alg:bp){reference-type="ref" reference="alg:bp"} describes
storing all messages separately (lines
[\[line:collect_store\]](#line:collect_store){reference-type="ref"
reference="line:collect_store"} and
[\[line:distribute_store\]](#line:distribute_store){reference-type="ref"
reference="line:distribute_store"}) and re-computing the full incoming
message products (final factor of lines
[\[line:collect_msg\]](#line:collect_msg){reference-type="ref"
reference="line:collect_msg"} and
[\[line:distribute_msg\]](#line:distribute_msg){reference-type="ref"
reference="line:distribute_msg"}) multiple times. This is somewhat
wasteful and we can in fact simply store a single product of all
incoming messages on each node, and when we wish to exclude a factor
from an outgoing message we simply divide by that factor. This detail
often reduces both the space and time required by the algorithm. This is
only possible because we have a well-defined division operator on
factors, which will not be the case when we discuss tensor propagation
in Chapter [\[cha:tp\]](#cha:tp){reference-type="ref"
reference="cha:tp"}. Other implementation tricks include normalisation
of messages for numerical stability, and sometimes working in log-space.

## 3.3 Junction tree algorithm {#sec:jt}

The fundamental limitation of the belief propagation algorithm as given
above is that it can only perform inference on trees. The junction tree
algorithm extends the belief propagation algorithm so it works for
general graphs. It is similar to the elimination algorithm, but like BP
is able to efficiently perform many queries at once. It consists of two
steps:

1.  Compile $G$ into a data structure called a *junction tree*

2.  Perform inference on the junction tree using message-passing as in
    BP.

We begin by describing the junction tree data structure, which is a
special type of *cluster graph*.

::: definition
Let $G$ be an MRF. A *cluster graph* over $G$ is an undirected graph
with nodes called *clusters* representing sets of variables
$\mathbf{C}_i \subset \scope{(G)}$. The clusters must be
*family-preserving*: each clique $\phi_c$ in $G$ must be associated with
a cluster $\mathbf{C}_i$ such that
$\scope{(\phi_c)} \subset \mathbf{C}_i$. A tree-structured cluster graph
is called a *cluster tree*.
:::

The intuition is that we assign the cliques of $G$ to groups, and each
group $\{\phi_j\}$ defines a cluster $\bigcup_j \scope{(\phi_j)}$
containing all the variables in the scope of its factors. Variables may
thus belong to more than one cluster. We define the *sepset* of an edge
in the cluster graph to be the set of common variables among the
clusters it joins.

::: definition
Each edge $(i,j)$ in a cluster graph is associated with a *sepset*
$\mathbf{S_{ij}} = \mathbf{C}_i \cap \mathbf{C}_j$.
:::

A *junction tree* is a cluster tree that satisfies one additional
property, called the running intersection property.

::: definition
A cluster tree has the *running intersection property* if for every
variable $X_i$ and pair of clusters $\mathbf{C}_a$ and $\mathbf{C}_b$,
$$X_i \in \mathbf{C}_a \cap \mathbf{C}_b \implies X_i \in \mathbf{C}_c \;\textrm{for all $\mathbf{C}_c$ in the path $\mathbf{C}_a \leadsto \mathbf{C}_b$}.$$
In other words, if a variable is in two clusters, it must also be in
every cluster on the path that connects the two clusters.
:::

### 3.3.1 Building the junction tree {#sec:jt_structure}

To build a junction tree for an MRF $G$, we use the following process.

1.  **Triangulate $\bm{G}$**. Add edges to $G$ to ensure every cycle
    with four nodes or more contains a chord (that is, an edge
    connecting two nodes in the cycle which itself is not part of the
    cycle).

2.  **Construct a cluster graph $\bm{\mathcal{H}}$**. $\mathcal{H}$
    should contain a cluster for each maximal clique in the triangulated
    $G$, with variable set equal to the scope of the clique. Add edges
    between all clusters that share at least one variable.

3.  **Construct a maximal spanning tree over $\bm{\mathcal{H}}$**.
    Weight each edge according to the number of variables in the sepset,
    and find the spanning tree with maximum total weight. This will
    satisfy the running intersection property and hence be a junction
    tree for $G$.

![Junction tree construction. Elimination order $f,d,e,c,b,a$ over the
MRF $G$ leads to a junction tree $T$ containing four clusters. Sepsets
are shown in rectangles. Image from [@paskinslides].](jt.png){#fig:jt
width="100%"}

#### 3.3.1.1 Triangulation {#sec:triangulation}

The triangulation step is necessary to ensure that the resulting cluster
tree satisfies the running intersection property. At a high level, we
are expanding the scope of the cliques in $G$ by adding edges to ensure
the junction tree satisfies this property. Note that this does not alter
the distribution represented by $G$. The triangulation for a given graph
$G$ is not unique, and must be chosen carefully as it has important
implications on the running time of inference. More precisely, the time
and space requirements of inference are exponential in the *induced
width* of the graph, equal to the size of the largest cluster in the
junction tree. We therefore seek the triangulation whose largest clique
is as small as possible. Determining the best triangulation of a graph
is $\mathcal{NP}$-hard. Various heuristics for triangulation are
discussed in [@koller09].

We define the *treewidth* of a graph $G$ to be one less than the size of
the largest max-clique in the best-case triangulation.[^3] Treewidth is
a central concept for PGMs because it determines the inherent complexity
of inference for a particular graph structure. For high-treewidth
graphs, inference is likely to be hard no matter what algorithm we use.
Since determining the best triangulation is $\mathcal{NP}$-hard,
determining the exact treewidth of a graph is also $\mathcal{NP}$-hard.

Another way to view the problem of choosing a triangulation is as
choosing an elimination order in the elimination algorithm (line
$\ref{line:elim_order}$ of Algorithm
[\[alg:elimination\]](#alg:elimination){reference-type="ref"
reference="alg:elimination"}). An elimination order defines a graph
triangulation, because the elimination cliques constructed during the
algorithm effectively add chords to the graph. Finding a good
triangulation is therefore equivalent to finding an elimination order
whose largest elimination clique is small. This equivalence is discussed
in more detail in [@koller09], but does not change the complexity
properties.

### 3.3.2 Junction tree inference

After construcing the junction tree, we perform inference in a similar
manner to belief propagation by recursively passing messages between
nodes. Messages are defined similarly as $$\begin{aligned}
    \m{t}{s}(\mathbf{x}_s) = \sum_{\mathbf{x}_t\setminus\mathbf{x}_s} \phi_{t}(\mathbf{x}_t) \prod_{u\neq s} \m{u}{t}(\mathbf{x}_t).\label{eq:jt_msg}\end{aligned}$$
Note that potential functions are now defined in terms of single
clusters. As in belief propagation, this recursion is well-defined
because the junction tree is singly connected. After all messages have
been computed, marginals are available for each cluster $\phi_s$ by
multiplying the cluster potential with all incoming messages, i.e.
$p(\mathbf{x}_s) \propto \phi_{s}(\mathbf{x}_s) \prod_{t} \m{t}{s}(\mathbf{x}_s)$.
Univariate marginals can be computed by brute force summation over
clique marginals, and normalised as required.

Equation ([\[eq:jt_msg\]](#eq:jt_msg){reference-type="ref"
reference="eq:jt_msg"}) involves summation over the cluster potential
$\phi_t(\mathbf{x}_t)$, which may have as many variables in its scope as
the induced width of the graph and therefore takes time exponential in
the induced width. This fact is why we desire small clusters, and why
exact inference with the junction tree algorithm rapidly becomes
infeasible as the treewidth becomes large.

#### 3.3.2.1 HUGIN and Shafer-Shenoy updates

We stated above that in belief propagation, we can maintain a single
product of incoming messages on each node rather than storing messages
individually. In the context of the junction tree algorithm, the former
is referred to as the *HUGIN* algorithm and the latter the
*Shafer-Shenoy* algorithm. HUGIN updates, where we store a product only
and divide as necessary, have space and time advantages over
Shafer-Shenoy updates in general but can be less efficient in certain
situations if new evidence is applied. As noted above, HUGIN updates
require a well-defined division operator on factors.

[^1]: In practice, during inference, it is often useful to re-scale
    factors regularly throughout the algorithm for numerical stability,
    so we do not necessarily ever compute $Z$ explicitly.

[^2]: A variant of belief propagation, called *loopy belief propagation*
    and sometimes simply belief propagation, is popular for approximate
    inference and can be run over arbitrary graphs. This algorithm is
    discussed in Section
    [\[sec:loopy_bp\]](#sec:loopy_bp){reference-type="ref"
    reference="sec:loopy_bp"}.

[^3]: We subtract one in the definition so that the treewidth of a tree
    is 1.
