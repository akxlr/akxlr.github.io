---
layout: post
title:  "Chapter 7"
main-title: "Conclusion and future work"
---

## 7.1 Conclusion

In this thesis, we have described a novel algorithm for approximate
inference in discrete graphical models, *low-rank tensor propagation*.
The algorithm approximates the junction tree algorithm by sampling
rank-one terms from the tensor rank decompositions of potential
functions, and using these to form low-rank approximations of messages.
We showed theoretically that this algorithm produces estimates of
univariate marginal distributions that are asymptotically consistent,
and that have encouraging asymptotic error properties as the running
time of the algorithm increases. In particular, our analysis suggests
that in some cases, we can avoid the exponential dependence on treewidth
for problems that are otherwise intractable. We then demonstrated
experimentally that on standard test cases, the tensor propagation
algorithm is able to outperform some of the most common approximate
inference methods.

## 7.2 Future work

This research suggests several interesting avenues for future work.

-   **Optimal tensor rank decomposition.** The most obvious unanswered
    question is precisely how one should perform the initial tensor rank
    decomposition on arbitrary clique potentials. The first part of this
    is developing a better understanding of exactly what properties of
    the decomposition cause it to perform well. Our analysis hints at
    some possibilities - minimising the magnitude of rank-one components
    compared to the function they represent, or minimising their
    variance. Future work could then investigate optimisation algorithms
    for tensor rank decomposition that incorporate these properties as
    their objective. These will be nonconvex optimisation problems that
    are likely to be difficult to solve in general, but may lend
    themselves to iterative approaches similar to ALS.

-   **Repeated sampling of marginals.** An interesting modification to
    the algorithm is to use a very small sample size $K$, but run the
    algorithm many times. This produces many marginal estimates instead
    of a single estimate, and therefore confers some potential
    advantages. Firstly, we can estimate the uncertainty in the marginal
    estimates using their sampling distribution. Secondly, the memory
    usage of the algorithm is much lower because we never need to
    maintain decompositions with a high number of terms. Finally, it
    would allow us to vary the runtime of the algorithm more flexibly,
    because we can simply run it for longer to obtain better results
    (rather than choosing the sample size $K$ in advance). For instance,
    one might choose to run the algorithm until the standard error of
    the marginal estimates is below some threshold.

-   **Deterministic low-rank algorithms.** One might also consider
    deterministic algorithms using low-rank approximations to messages.
    Specifically, one possibility is to perform approximate
    multiplications by optimising the product for some objective given
    its operands, rather than sampling. It should be possible to find
    low-rank approximations to products that approximate the full-rank
    products optimally in some sense. This introduces an additional
    (potentially expensive) optimisation step at each multiplication,
    and we would likely lose the guarantee of unbiasedness, but may
    recover better marginals overall.

-   **Applications to learning.** Improved methods of approximate
    inference, such as presented in this thesis, also have important
    implications for learning (that is, finding the best set of
    parameters for a given graphical model so that the distribution
    matches that of a given set of training data). For example, in
    restricted Boltzmann machines (RBMs) used to build deep belief
    networks, the fundamental difficulty in learning parameters of the
    model is due to the difficulty of approximating the expected value
    of the distribution under a current parameter set. This is
    computationally identical to the problem of marginalisation, and
    therefore our approach could potentially improve learning in such
    models.

-   **Continuous models.** Finally, one of they key practical
    difficulties of PGMs is dealing with continuous distributions, which
    are found in many real-world distributions. Inference methods based
    on sampling are naturally applicable to continuous distributions and
    our approach may therefore lead to improvement in these models given
    suitable modifications.
 
