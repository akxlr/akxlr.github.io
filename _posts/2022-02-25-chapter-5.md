---
layout: post
title:  "Chapter 5"
main-title: "Low-rank tensor propagation"
### Bibliography ###
lafferty2001conditional: "Lafferty et al. 2001"
liu2006conditional: "Liu 2006"
gog13: "Gogate and Domingos 2013"

jordan2003introduction: "Jordan 2003"
koller09: "Koller and Friedman 2009"
len20: "Lenz 1920"
isi25: "Ising 1925"
bis06: "Bishop 2006"
dom14: "Domke 2014"
pit36: "Pitman 1936"
besag1974spatial: "Besag 1974"
kolda09: "Kolda and Bader 2009"
nickthesis: "Vannieuwenhoven 2015"
koldatoolbox: "Bader et al. 2015"
lee01: "Lee and Seung 2001"
hoe63: "Hoeffding 1963"
### end of Bibliography ###
---


We now describe our main contribution, *low-rank tensor propagation*.
This is an approximate inference algorithm for discrete graphical
models. Our algorithm is based on the junction tree algorithm, but
addresses the intractability of large multiplication and marginalisation
operations by representing potential functions in a particular way,
namely as sums of rank-one tensors. We show that by sampling terms from
the decompositions, we can approximate large computations in an unbiased
manner to achieve marginal estimates that are asymptotically consistent.
We provide analysis, giving sufficient conditions for the algorithm to
perform well including when the treewidth of the input graph is large.

## 5.1 Tensor rank decomposition {#sec:cp}

Given a discrete MRF $G$, each potential function $\phi_c$ can be
represented naturally in tabular form by a multidimensional array
$\mathbf{T}\_c$, i.e. a *tensor*. Specifically, if
$\scope{(\phi_c)}=\sublist{X}{\mathcal{D}}$ and the domain of each $X_i$
is $$\{1,\ldots,N_i\}$$ then
$\mathbf{T}\_c \in \mathbb{R}^{N_1 \times \cdots \times N_{\mathcal{D}}}$
where
$\mathbf{T}_{\sublist{i}{\mathcal{D}}} = \phi_c(\sublist{i}{\mathcal{D}})$.
We say $\mathbf{T}$ is *order*-$\mathcal{D}$. In this section, we
describe a decomposition applicable to arbitrary tensors, called the
*tensor rank decomposition*, which we use to decompose potential
functions in our inference algorithm. We begin by stating two necessary
definitions.


(Outer product). The *outer product* of a set of 
$\mathcal{D}$ vectors is denoted
$\mathbf{a}_1 \otimes \cdots \otimes \mathbf{a}\_{\mathcal{D}}$ and is
the ${\mathcal{D}}$-dimensional tensor such that
{:.def text='5.1.1'}

{% raw %}
$$\left(\mathbf{a}_1 \otimes \cdots \otimes \mathbf{a}_{\mathcal{D}}\right)_{\sublist{i}{{\mathcal{D}}}} = \left(\mathbf{a}_1\right)_{i_1} \cdot \cdots \cdot \left(\mathbf{a}_{\mathcal{D}}\right)_{i_{\mathcal{D}}}$$

for all valid indices $\sublist{i}{{\mathcal{D}}}$.
{% endraw %}

(Rank-one tensor). A *rank-one* or *separable* tensor is a tensor
$\mathbf{T} \in \mathbb{R}^{N_1 \times \cdots \times N_{\mathcal{D}}}$
that can be expressed as an outer product of vectors, i.e.
$\mathbf{T} = \mathbf{a}_1 \otimes \cdots \otimes \mathbf{a}\_{\mathcal{D}}$.
{:.def text='5.1.2'}

Every tensor, and therefore each discrete potential function, admits the
following decomposition.


(Tensor rank decomposition). Let
$\mathbf{T} \in \mathbb{R}^{N_1 \times \cdots \times N_{\mathcal{D}}}$
be a real tensor. A *tensor rank decomposition* of $\mathbf{T}$ is an
expression of the form 
{:.def text='5.1.3'}

$$
\begin{align}
    \mathbf{T} = \sum_{k=1}^r w_k \; \mathbf{a}_k^1 \otimes \mathbf{a}_k^2 \otimes \cdots \otimes \mathbf{a}_k^{\mathcal{D}},\quad w_k\in\mathbb{R}, \quad \mathbf{a}_k^i \in \mathbb{R}^{N_i} \tag{5.1}\label{eq:cp}
\end{align}
$$

where $r$ is minimal. $r$ is called the *rank* of $\mathbf{T}$. For
identifiability, and for the probabilistic interpretation to follow, we
assume $\sum_kw_k=1$.

The decomposition (\ref{eq:cp}) is known by several names reflecting its history,
including CANDECOMP/PARAFAC (CP) decomposition (meaning "Canonical
decomposition/parallel factors"), canonical polyadic decomposition (CPD)
and Hitchcock decomposition [{{page.kolda09}}]. The idea is to build up
$\mathbf{T}$ as a weighted sum of rank-one tensors. It is conceptually
similar to the singular value decomposition (SVD) for matrices, which
can be seen as decomposing a matrix into a sum of rank-one matrices
weighted by the singular values.

For a potential function {% raw %} $\phi_c(\sublist{x}{{\mathcal{D}}})$ {% endraw %} over ${\mathcal{D}}$ variables, (\ref{eq:cp}) is equivalent to decomposing $\phi_c$ into a sum of
fully-factorised terms, i.e. 

{% raw %}
$$
\begin{align}
    \phi_c(\sublist{x}{{\mathcal{D}}}) &= \sum_{k=1}^{r} w_k^c\; \psi_{k}^c(\sublist{x}{{\mathcal{D}}}) \nonumber \\
    &= \sum_{k=1}^{r} w_k^c\; \psi_{k,1}^c(x_1) \psi_{k,2}^c(x_2) \cdots \psi_{k,{\mathcal{D}}}^c(x_{\mathcal{D}}). \tag{5.2}\label{eq:fact}
\end{align}
$$
{% endraw %}

In other words, each rank-one tensor is equivalent to a fully-factorised
function. We use this form when discussing marginalisation and
multiplication of decomposed potentials, and will generally omit the
superscript $c$ for clarity.

### 5.1.1 Existence and uniqueness

It is trivial to show that the decomposition
(\ref{eq:cp}) always
exists: let $r=\prod_{i=1}^{\mathcal{D}} N_i$ so each term corresponds
to a single cell in $\mathbf{T}$, let $w_k$ be the value of that cell
and let the $\mathbf{a}_k^i$ be standard basis vectors selected so that
their outer product is zero everywhere except at that cell, where it is
one. If $\sum_kw_k \neq 1$, normalise so $\sum_k w_k=1$ and rescale the
basis vectors. Tensors generally admit much more compact representations
than this, though an exact expression for the maximal rank that can be
obtained by an order-${\mathcal{D}}$ tensor is not presently known. In
contrast to (order 2) matrices, higher-order tensor rank decompositions
are often, but not always, unique (up to scaling and re-ordering)
[{{page.kolda09}}; {{page.nickthesis}}].

### 5.1.2 Computation using ALS

Computing the tensor rank decomposition for a general tensor
$\mathbf{T}$ is nontrivial; determining even the rank of $\mathbf{T}$ is
$\mathcal{NP}$-hard. In practice, to decompose a tensor $\mathbf{T}$ we
fix $r=1$ in (\ref{eq:cp}), use numerical methods to find a rank-$r$
decomposition that is "good", and repeat with increasing $r$ until we
get an acceptably low (or zero) error. The main algorithm used to find
the best decomposition for fixed $r$ is *alternating least squares*
(ALS), which attempts to minimise Euclidean distance to the
approximation. This problem is stated as 

$$
\begin{align}
    \min_{\mathbf{T}_r} \Vert(\mathbf{T}_r, \mathbf{T})\Vert_F^2 \quad \textrm{such that} \quad \mathbf{T}_r = \sum_{k=1}^r w_k \; \mathbf{a}_k^1 \otimes \cdots \otimes \mathbf{a}_k^{\mathcal{D}} \tag{5.3}\label{eq:cp_opt}
\end{align}
$$
where $\Vert\cdot\Vert_F$ is the Frobenius (Euclidean) norm.[^1] After
initialising all the $$\{\mathbf{a}_k^i\}$$ and $$\{w_k^i\}$$ with some
starting guess, the ALS algorithm starts by fixing $$\mathbf{a}_k^i$$ for
all $i\neq 1$ and all $k$. We then solve for the first set of vectors
$$\{\mathbf{a}_k^1\}_{k=1}^r$$ by treating ($\ref{eq:cp_opt}$) as a linear
least-squares problem. This sub-problem can be solved quickly and
optimally via an analytic solution using well-known matrix operations.
We then repeat but instead fix $$\mathbf{x}_k^i$$ for $i\neq 2$ and
optimise for the second set of vectors $$\{\mathbf{a}_k^2\}_{k=1}^r$$. We
continue looping through the $i$ in this manner until we reach some
convergence criteria (a sufficiently small change in the objective,
small change in the component tensors, pre-defined iteration limit,
etc). On convergence, the ALS algorithm does not necessarily reach an
optimal solution [{{page.kolda09}}; {{page.nickthesis}}].

Note, however, that even if we could solve (\ref{eq:cp_opt}) optimally, this does not necessarily lead to the
best decomposition for our purposes. There are additional properties of
the decomposition that affect the performance of our tensor propagation
algorithm, discussed in our analysis below.

### 5.1.3 Decomposition for Ising models

In the case of Ising models, and any pairwise MRFs with Ising potentials
of the form $\phi_{ij}(x_i,x_j) = \exp (w_{ij}x_i x_j)$, the $2\times 2$
potential tables are in general rank-2 of the form

$$\begin{pmatrix}\theta & \frac{1}{\theta}\\ \frac{1}{\theta} & \theta\end{pmatrix}, \quad \theta=\exp(w_{ij}).$$

For tables of this particular form, we note there is a natural rank-2
decomposition that one can compute quickly by assuming terms in the
decomposition are symmetric, by solving

$$\begin{pmatrix}\theta & \frac{1}{\theta}\\ \frac{1}{\theta} & \theta\end{pmatrix} = \begin{pmatrix}a\\b\end{pmatrix} \otimes \begin{pmatrix}b\\a\end{pmatrix} + \begin{pmatrix}b\\a\end{pmatrix} \otimes \begin{pmatrix}a\\b\end{pmatrix}.$$

Specifically, solving 

$$
\begin{aligned}
    2ab &= \theta \\
    a^2 + b^2 &= \frac{1}{\theta}
\end{aligned}
$$ 

gives 

$$
\begin{align}
a &= \frac{1}{2} \left( \sqrt{\theta+\frac{1}{\theta}} \pm \sqrt{\theta-\frac{1}{\theta}} \right), & b &= \frac{1}{2} \left( \sqrt{\theta+\frac{1}{\theta}} \mp \sqrt{\theta-\frac{1}{\theta}} \right) \quad \textrm{or} \nonumber \\
a &= \frac{1}{2} \left( -\sqrt{\theta+\frac{1}{\theta}} \pm \sqrt{\theta-\frac{1}{\theta}} \right), & b &= \frac{1}{2} \left( -\sqrt{\theta+\frac{1}{\theta}} \mp \sqrt{\theta-\frac{1}{\theta}} \right) \tag{5.4}\label{eq:symm_decomp}.
\end{align}
$$

While this decomposition is of limited usefulness for general models, it
allows us to avoid the ALS optimisation in our experiments and thus
explore the properties of low-rank tensor propagation independently of
our ability to find good initial rank-decompositions. We use the first
solution in our experiments.

### 5.1.4 Data structure

In our implementation, weights are stored in a vector $\mathbf{w}$. For
the factors of the decomposition, we store a matrix $\mathbf{A}\_i$ for
each variable $X_i$ containing values of the factor $\psi_{k,i}$ for all
$k$, i.e. $(\mathbf{A}\_i)\_{pq} = \psi_{q,i}(p)$.

## 5.2 Operations using decomposed functions

We now describe how one can use the tensor rank decomposition to perform
efficient exact marginalisation and approximate multiplication of
potential functions.

### 5.2.1 Marginalisation {#sec:tp_marg}

Let {% raw %} $\phi(\sublist{x}{{\mathcal{D}}})$ {% endraw %}
 be a potential function where $$x_i\in\{1,\ldots ,N_i\}$$ for each $i$, and suppose we wish to
marginalise out a single variable $X_j$ by computing $\sum_{x_j}\phi$.
As discussed in Section 2.1, the running time of this operation is
naïvely $\bigO{\prod_i N_i}$. In contrast, if $\phi$ is stored in
decomposed form with $r$ terms, we can compute the exact result in
$\bigO{rN_j}$ time. Each term in the decomposition is fully factorised,
so we can express the marginalisation as 

{% raw %}
$$
\begin{aligned}
    \sum_{x_j} \phi(\sublist{x}{{\mathcal{D}}}) &= \sum_{x_j} \sum_{k=1}^{r} w_k\; \psi_{k,1}(x_1) \psi_{k,2}(x_2) \cdots \psi_{k,{\mathcal{D}}}(x_{\mathcal{D}}) \\
    &= \sum_{k=1}^{r} w_k\; \left(\sum_{x_j} \psi_{k,j}(x_j)\right) \underbrace{\psi_{k,1}(x_{1}) \psi_{k,2}(x_{2}) \cdots \psi_{k,{\mathcal{D}}}(x_{s})}_\text{excluding $\psi_{k,j}(x_j)$}
\end{aligned}
$$
{% endraw %}

where we push the sum $\sum_{x_j}$ inside and only evaluate it over the
univariate factor $\psi_{k,j}(x_j)$. We then absorb this sum into the
weights $$\{w_k\}$$ so the result stays in decomposed form
(\ref{eq:fact}). The procedure for marginalising out $X_j$ is thus simply 

$$
\begin{align}
w_k \gets w_k \cdot \sum_{x_j} \psi_{k,j}(x_j) \quad \textrm{for all $k$} \tag{5.5}\label{eq:cp_marg}.
\end{align}
$$

Computationally, this is dominated by $\sum_{x_j} \psi_{k,j}(x_j)$ which
is a sum over rows of the $N_j\times r$ matrix $\mathbf{A}_j$, and has
complexity $\bigO{rN_j}$. To marginalise over multiple variables, we
evaluate (\ref{eq:cp_marg}) for each variable in turn.

### 5.2.2 Multiplication

Exact multiplication of two $r$-term decomposed potentials requires
$r^2$ terms in the product, which quickly becomes infeasible as messages
are propagated through the graph during inference. In this section, we
describe a method of approximating this $r^2$-term product with less
terms.

#### 5.2.2.1 Setup

Let $\phi_a$ and $\phi_b$ be $r$-term decomposed potential functions
over ${\mathcal{D}}_a$ and ${\mathcal{D}}_b$ variables respectively,
i.e. $\vert\scope{(\phi_a)}\vert={\mathcal{D}}_a$ and
$\vert\scope{(\phi_b)}\vert={\mathcal{D}}_b$. Let
$\sublist{X}{\mathcal{D}} = \scope{(\phi_a)} \cup \scope{(\phi_b)}$ be
the set of variables contained in the combined scope, and suppose
$$X_i\in\{1,\ldots ,N_i\}$$ for each $X_i$. Our task is to approximate the
product $\phi_a \phi_b$.

#### 5.2.2.2 Rank decomposition as a probability distribution

The decomposition (\ref{eq:fact}) defines a probability distribution over its
rank-one terms $$\{\psi_k\}$$ in a natural way, by choosing each term $k$
with probability $w_k$. Formally, given decomposed potential
$\phi(\mathbf{x}) = \sum_k w_k \psi_k(\mathbf{x})$, let $\mathcal{K}$ be
a random variable with $P(\mathcal{K}=k)=w_k$. Then 

$$
\begin{align}
    \hat\phi(\mathbf{x}) := \frac{1}{K}\sum_{i=1}^K \psi_{\mathcal{K}_i}(\mathbf{x}) \tag{5.6}\label{eq:estimator}
\end{align}
$$

is an unbiased estimator of $\phi(\mathbf{x})$, since

$$\mathbb{E}{[}{\hat\phi}{]} = \frac{1}{K}\sum_i\mathbb{E}{[}{\psi_{\mathcal{K}_i}}{]} = \mathbb{E}{[}{\psi_{\mathcal{K}_i}}{]} = \sum_k w_k \psi_k = \phi.$$

#### 5.2.2.3 Multiplication by sampling

We use this fact to perform approximate multiplication in the most
obvious way: sample a term from each operand, multiply these terms
together, and repeat to add as many terms as desired to the product.
This process is detailed in [Algorithm 3](#alg:alg3). To simplify our discussion, we consider the
case of multiplying two operands only; extension to multiple operands is
straightfoward.

![alg3](/assets/images/alg_3.jpg)
{:.img.fig.alg_3 style="width: 100%" id="alg:alg3"}

<br/>

We begin by drawing a sample of size $$K$, $\{(k_i,l_i)\}_{i=1}^K$$, where
the $k_i$ and $l_i$ are drawn according to the weights in the respective
decompositions. The weights in the estimated product are then the
normalised frequencies of each sampled term pair. To compute the
rank-one terms of the estimated product, we must compute
$\psi^a_{k_i} \psi^b_{l_i}$ for each sampled pair which can be performed
quickly due to the factorised forms. For example, multiplying two
3-variable rank-one terms with two common variables can be expressed as

$$
\begin{aligned}
    \psi_a(v,x,y)\psi_b(x,y,z) &= \psi_{a,1}(v)\psi_{a,2}(x)\psi_{a,3}(y)\psi_{b,1}(x)\psi_{b,2}(y)\psi_{b,3}(z) \\
    &= \underbrace{\psi_{a,2}(x)\psi_{b,1}(x)}_\text{$X$ factors} \underbrace{\psi_{a,3}(y) \psi_{b,2}(y)}_\text{$Y$ factors} \underbrace{\psi_{a,1}(v) \psi_{b,3}(z)}_\text{other factors}.
\end{aligned}
$$

The time required for this operation is $\bigO{\sum_i N_i}$, and thus
the time complexity of multiplication is $\bigO{K\sum_iN_i}$. By
comparison, exact multiplication using a tabular representation requires
$\bigO{\prod_i N_i}$ operations. To extend [Algorithm 3](#alg:alg3) to more than two operands, we can simply
sample from each operand in turn and multiply the resulting rank-one
tensors in the same manner.

Note that for sample size $K$, the resulting product has at most $K$
terms but generally significantly less, since many terms are sampled
twice. Therefore, we can set $K$ substantially higher than the desired
number of terms in the product.

#### 5.2.2.4 Unbiasedness

If $\phi_a$ and $\phi_b$ both have $r$-term decompositions, then their
exact $r^2$-term product is

$$\phi_a\phi_b = \sum_k \sum_l w^a_k w^b_l \psi^a_k \psi^b_l.$$

If we wanted to draw samples from this product, we would choose each term
$\psi^a_k\psi^b_l$ with probability $w^a_k w^b_l$, and as stated above
this is an unbiased estimate of the true product. Note that this is
identical to drawing samples from each operand independently and then
multiplying them together, hence Algorithm
[Algorithm 3](#alg:alg3) provides an unbiased estimate of the true
product. Phrased differently, since the two sampled terms are
independent unbiased estimates of their operands, their product is an
unbiased estimate of the product $\phi_a \phi_b$.

## 5.3 Low-rank tensor propagation {#low-rank-tensor-propagation}

Using the above exact marginalisation and approximate multiplication, we
propose the following *low-rank tensor propagation* algorithm, which we
will henceforth abbreviate to *tensor propagation* or TP. Given an MRF
$G$:

1.  For each clique potential $\phi_c$, compute a tensor rank
    decomposition.

2.  Using the decomposed potentials, construct a junction tree over G,
    using approximate multiplication for large products. The resulting
    cluster potentials will be in decomposed form.

3.  Run the junction-tree inference algorithm described in Section 3.3 with
    Shafer-Shenoy updates, but

    -   perform exact marginalisation efficiently using the decomposed
        representations;

    -   use approximate multiplication for large products;

    -   apply reweighting before each multiplication (see below).

We define large multiplications to be multiplications whose exact
product would have more than $H$ terms for some $H$ (for smaller
multiplications, we still maintain functions in decomposed form but can
simply compute the $r^2$-term product exactly with a nested loop). The
main parameters that control the behaviour of the algorithm are thus the
number of samples per approximate multiplication $K$, $H$, and the
reweighting scheme used as described below. We test the effect of each
of these parameters in [Chapter 6]({% link _posts/2022-02-26-chapter-6.md %}). Note that we must use Shafer-Shenoy
updates, rather than HUGIN, because we no longer have a well-defined
division operation on factors.

## 5.4 Analysis {#sec:analysis}

In this section we establish two main results. First, we show that the
marginal estimates returned by TP are asymptotically consistent. Second,
we characterise the number of samples $K$ required to give results
accurate to within a certain relative error.

### 5.4.1 Setup {#sec:analysis_setup}

We consider a single run of the tensor propagation algorithm. To
simplify our analysis, we consider only binary MRFs, that is, MRFs where
each variable can take two values. We also assume that all
multiplication operations have only two operands, and larger
multiplications are performed by repeated two-way multiplications. Our
results can be generalised to arbitrary MRFs and n-way multiplications.

Let $G$ be an MRF with $C$ max-cliques describing the distribution

$$p(\sublist{X}{\mathcal{D}}) = \frac{1}{Z} \prod_{c=1}^C \phi_c(\mathbf{x}_c)$$

and suppose all clique potentials $\phi_c$ are given in decomposed form.
Denote the univariate marginal of variable $X_i$ as $p_i(x_i)$. Let
$$d(\mathbf{x})=\prod_{c\in\cl(G)} \phi_c(\mathbf{x}_c)$$ be the
unnormalised distribution associated with the MRF and
$d_i(x_i) = \sum_{\mathbf{x}\setminus x_i}d(\mathbf{x})$ be the
unnormalised marginals. Suppose the junction tree constructed by the
algorithm has induced width $T$ and contains $Q$ clusters.

We will use the term *$M$-bounded function* to refer to a decomposed
function whose rank-one components contain entries no greater than $M$.
Assume that all function estimates constructed throughout the algorithm
are $M$-bounded for some constant $M>0$. Note that such an $M$ always
exists that is independent of the sample size $K$.

#### 5.4.1.1 Non-negativity

For the remainder of this paper, we assume the decompositions are
non-negative, that is, that each rank-one tensor contains non-negative
entries. It is possible to run the algorithm with arbitrary
decompositions, but this necessarily results in a nonzero probability of
some approximations containing negative entries (i.e.
$\phi(\mathbf{x}) = \sum_k w_k \psi_k(\mathbf{x}) < 0$ for some
$\mathbf{x}$) due to sampling variance. Among other problems, this
causes issues with normalisation during the algorithm. In our
experiments, using negatives was problematic unless an extremely high
sample size was used.

A general algorithm for non-negative tensor rank decomposition is
implemented in [{{page.koldatoolbox}}], which makes use of the non-negative
matrix factorisation updates in [{{page.lee01}}]. We do not use this in our
experiments, however, since we use the simpler case above for Ising
potentials.

### 5.4.2 Consistency of tensor propagation

We first show that our algorithm is consistent, that is, that the
marginal estimates converge in probability to the true marginals as the
sample size $K$ approaches infinity.[^2] To do this, we will make use of
the following concentration bound due to Hoeffding [{{page.hoe63}}]:

(Hoeffding’s inequality). Let $\sublist{X}{K}$ be iid random variables in the range $[0,M]$, and
let $\overline X = \frac{1}{K}\sum_{i=1}^K X_i$. Then for $t>0$,
{:.theorem text='5.4.1'}

$$P(\vert \overline X - \mathbb{E}{[}{\overline X}{]} \vert \geq t) \leq 2\exp \left(-\frac{2Kt^2}{M^2}\right).$$


We begin by stating the following lemma, which bounds the total number
of invocations of the $\textsc{Multiply}$ procedure.

A complete execution of TP makes
$\bigO{C^2}$ $\textsc{Multiply}$ calls.
{:.lemma text='5.4.1' id='lem:541'}

*Proof.* To construct the junction tree, the total number of binary
multiplications required is $\bigO{C}$, since each clique potential is
multiplied onto its respective cluster once. During inference, a message
is passed along each edge in the junction tree in both directions, so
the number of messages passed is $\bigO{Q}$. Construction of each
message involves a product of no more than $Q-2$ incoming messages and a
local potential, and hence takes $\bigO{Q}$ binary multiplications.
Thus, inference requires $\bigO{Q^2}$ binary multiplications and
therefore the total number of $\textsc{Multiply}$ calls is
$\bigO{Q^2+C}$. Since $Q < C$, this is $\bigO{C^2}$.

A complete execution of TP makes
$\bigO{C^2 2^T}$ estimates of individual table cells.[^3]
{:.corr text='5.4.1' id='corr:541'}

*Proof.* By inspection of the message passing rule
3.2,
no multiplication creates a factor with scope larger than any individual
cluster, which contain at most $T$ variables by the definition of the
induced width $T$. Thus, every multiplication estimates a table with at
most $2^T$ cells, and by [Lemma 5.4.1](#lem:541) the total number of estimates is therefore
$\bigO{C^2 2^T}$.

We can now prove our consistency result.


(Consistency of TP). Let
$$\{\hat{d_i}(x_i)\}_{i=1}^\mathcal{D}$$ be the unnormalised marginal
estimates obtained by the TP algorithm, and
$$\{\hat{p_i}(x_i)\}_{i=1}^\mathcal{D}$$ the corresponding estimates after
normalisation. Then $\hat d_i(x_i)$ is a consistent estimator of
$d_i(x_i)$, and $\hat p_i(x_i)$ is a consistent estimator of $p_i(x_i)$,
for all $i$ and all $x_i$.
{:.theorem text='5.4.2' id="th:542"}


*Proof.* We will show that for any $\epsilon > 0$,
$\lim_{K\to\infty} P(\vert\hat d_i(x_i) - d_i(x_i)\vert \geq \epsilon) = 0$
for all $i$ and $x_i$. We prove this by applying Hoeffding's inequality
to each of the $\bigO{C^2 2^T}$ estimates, then applying a union bound
over all estimates.

Per [Corollary 5.4.1](#corr:541), let $\alpha$ be a constant such that
the number of individual table cell estimates made by an entire
execution of TP is less than $\alpha C^2 2^T$. Since each of these
estimates is in the range $[0,M]$, each satisfies Hoeffding's inequality
with respect to its expected value.[^4] Applying a union bound[^5] to
these estimates gives a bound on the probability that any estimate is
outside $t$ from its expected value: 

$$
\begin{align}
    P\left(\left| \overline Y_j - \mathbb{E}{[}{\overline Y_j}{]} \right| \geq t  \; \textrm{for any estimate $j$} \right) &\leq \alpha C^2 2^T \exp \left(-\frac{2Kt^2}{M^2}\right) \tag{5.7}\label{eq:h_eps_bound}
\end{align}$$

where $\overline Y_j$ denotes the estimate made by the
[Multiply]{.smallcaps} procedure of some table cell $j$. If
$\left| \overline Y_j - \mathbb{E}{[}{\overline Y_j}{]} \right| \leq t$ for all
estimates $j$, we claim that
$\vert \hat d_i(x_i) - d_i(x_i) \vert \leq 2^{TQ}(2M)^{Q^2+C}t$ for all
$i$ and $x_i$. To show this, we examine the worst case where all
estimates are $\pm t$ from their expected value. We will assume WLOG
that all estimates are $+t$ from their expected value, and examine how
the error grows as messages are propagated through the junction tree.
For a single multiplication step, the following lemma applies.

::: lemma
[]{#lem:l2 label="lem:l2"} Let $\phi_1$ and $\phi_2$ be $M$-bounded
functions and $0<t\leq M$. Consider any cell $r$ in the table of product
$\phi_1\phi_2$, and its corresponding cell $r'$ in the table of
$(\phi_1 + \beta t)(\phi_2 + t)$ for some $\beta>0$. The error satisfies
$r' - r \leq M(1+2\beta)t$.[^6]
:::

::: proof
*Proof.* Each cell $r'$ is computed as a product $(a + \beta t)(b + t)$,
where $a$ is some entry of $\phi_1$, $b$ some entry of $\phi_2$, and
$r=ab$. We observe $$\begin{aligned}
        (a + \beta t)(b + t) &= ab + at + \beta t b + \beta t^2 \\
        &\leq ab + Mt + \beta Mt + \beta Mt \\
        &= ab + Mt(1 + 2\beta)
    \end{aligned}$$ Thus,
$r' - r \leq ab + Mt(1+2\beta) - ab = Mt(1+2\beta)$ as required. ◻
:::

::: corr
[]{#corr:c2 label="corr:c2"} Suppose we have $F$ $M$-bounded functions
$\phi_1,\ldots,\phi_F$. Let $r$ be any cell in the product
$\phi_1 \cdots \phi_F$, and $r'$ the corresponding cell in the product
$(\phi_1+t)\cdots (\phi_F+t)$ computed by repeatedly invoking the binary
[Multiply]{.smallcaps} procedure. Then $r' - r \leq (2M)^Ft$.
:::

::: proof
*Proof.* We prove the tighter bound $r'-r \leq (2^F-1)M^{F-1}t$ by
induction on $F$. The error for $F=2$ after performing the first
multiplication $(\phi_1+t)(\phi_2+t)$, which we denote $r_2'-r_2$, is no
greater than $3Mt$ since we can invoke Lemma
[\[lem:l2\]](#lem:l2){reference-type="ref" reference="lem:l2"} with
$\beta=1$. If the error after $F$ multiplications, $r_F'-r_F$, is no
greater than $(2^F-1)M^{F-1}t$, then the error after $F+1$
multiplications, $r'_{F+1}-r_{F+1}$, is bounded by invoking Lemma
[\[lem:l2\]](#lem:l2){reference-type="ref" reference="lem:l2"} with
$\beta=(2^F-1)M^{F-1}$, and hence is no greater than
$$Mt(1+2[(2^F-1)M^{F-1}]]) \leq Mt(M^{F-1}+2[(2^F-1)M^{F-1}]]) = (2^{F+1}-1)M^Ft.$$
Thus, the looser bound $r' - r \leq (2M)^Ft$ holds. ◻
:::

Consider a single cluster node of the junction tree. To compute an
outgoing message during inference, we compute
([\[eq:jt_msg\]](#eq:jt_msg){reference-type="ref"
reference="eq:jt_msg"}), i.e. $$\begin{aligned}
    \m{t}{s}(\mathbf{x}_s) = \sum_{\mathbf{x}_t\setminus\mathbf{x}_s} \phi_{t}(\mathbf{x}_t) \prod_{u\neq s} \m{u}{t}(\mathbf{x}_t).\end{aligned}$$
The multiplication
$\phi_{t}(\mathbf{x}_t) \prod_{u\neq s} \m{u}{t}(\mathbf{x}_t)$ involves
no more than $Q$ terms, and we compute the result by repeatedly calling
[Multiply]{.smallcaps}. By Corollary
[\[corr:c2\]](#corr:c2){reference-type="ref" reference="corr:c2"}, if
the multiplicands $\phi_t$ and $\m{u}{t}$ each have error $t$, then the
result of the multiplication has error at most $(2M)^Qt$. To compute the
outgoing message, we must then perform the marginalisation step
indicated by the sum. In general, the resulting message $\m{t}{s}$ is a
sum of no more than $2^T$ terms, since the product table we are summing
over has no more than $2^T$ terms. Thus, if the incoming messages to a
node each have error $t$, then the outgoing message will have error no
more than $2^T(2M)^Qt$.

Each unnormalised marginal estimate $\hat d_i(x_i)$ depends on the
upstream messages for the tree rooted at $X_i$ of which there are no
more than $Q$. Thus, the above outgoing message computations compound
the error no more than $Q$ times. Furthermore, in the initial junction
tree construction step, each cluster potential is computed by
multiplying up to $C$ clique potentials and thus each cluster potential
has error no more than $(2M)^Ct$ by Corollary
[\[corr:c2\]](#corr:c2){reference-type="ref" reference="corr:c2"}.
Therefore, if every estimate in the TP algorithm is within $t$ of its
expected value, the final marginal estimates satisfy
$\vert \hat d_i(x_i) - d_i(x_i) \vert \leq (2^T(2M)^Q)^Q\cdot (2M)^Ct = 2^{TQ}(2M)^{Q^2+C}t$.

We have shown that
$$\left| \overline Y_j - \E{\overline Y_j} \right| \leq t  \; \textrm{for all estimates $j$} \implies \vert \hat d_i(x_i) - d_i(x_i) \vert \leq 2^{TQ}(2M)^{Q^2+C}t \; \textrm{for all}\; i, x_i.$$
Therefore, by
([\[eq:h_eps_bound\]](#eq:h_eps_bound){reference-type="ref"
reference="eq:h_eps_bound"}), $$\begin{aligned}
%     &P\left(\left| \overline Y_j - \mu_j \right| \geq t  \; \textrm{for any estimate $j$} \right) \leq \alpha C^2 2^T \exp \left(-\frac{2Kt^2}{M^2}\right) \label{eq:h_eps_bound} \\
%     \implies &
    P\left(\vert \hat d_i(x_i) - d_i(x_i) \vert \geq 2^{TQ}(2M)^{Q^2+C}t \; \textrm{for any}\; i, x_i\right) \leq \alpha C^2 2^T \exp \left(-\frac{2Kt^2}{M^2}\right)\end{aligned}$$
To complete the proof, suppose $\epsilon$ is given and let
$t = \frac{\epsilon}{2^{TQ}(2M)^{Q^2+C}}$. Then $$\begin{aligned}
    P\left(\vert \hat d_i(x_i) - d_i(x_i) \vert \geq \epsilon \; \textrm{for any} \; i, x_i\right) \leq \alpha C^2 2^T \exp \left(-\frac{2K\epsilon^2}{M^2 2^{2TQ}(2M)^{2(Q^2+C)}}\right). \label{eq:h_bound}\end{aligned}$$
The limit as $K{\to}\infty$ of the RHS is 0, establishing that the
$\hat d_i(x_i)$ are consistent estimators of the $d_i(x_i)$. To show
consistency of the $\hat p_i(x_i)$, note that
$$\hat p_i(x_i) = \frac{\hat d_i(x_i)}{\sum_{x_i} \hat d_i(x_i)}.$$
Since the $\hat d_i(x_i)$ are consistent, consistency of $\hat p_i(x_i)$
is a direct result of Slutsky's theorem.[^7] ◻
:::

### 5.4.3 Multiplicative bound

The bound ([\[eq:h_bound\]](#eq:h_bound){reference-type="ref"
reference="eq:h_bound"}) is sufficient to prove consistency of TP, but
is otherwise not particularly useful in explaining the behaviour of the
algorithm due to its looseness. In this section, we derive a tighter
bound that gives a better insight into how sample size affects error in
practice, under the additional condition that no table cell estimates
have expected value $0$. Our analysis is similar to the above, but we
use a different concentration bound, namely the multiplicative Chernoff
bound [@ron08].

::: theorem
Let $\sublist{Y}{K}$ be iid random variables in range $[0,M]$, and let
$\overline Y = \frac{1}{K}\sum_{i=1}^K Y_i$. For any small $t>0$,
$$\begin{aligned}
    P\left(\overline Y \leq (1-t)\E{\overline Y}\right) &\leq \exp \left(-\frac{t^2\E{\overline Y} K }{2M}\right),\;\textrm{and} \\
    P\left(\overline Y \geq (1+t)\E{\overline Y}\right) &\leq \exp \left(-\frac{t^2\E{\overline Y} K }{3M}\right)\end{aligned}$$
and therefore $$\begin{aligned}
    P\left(\overline Y \notin \left[(1-t)\E{\overline Y}, (1+t)\E{\overline Y}\right] \right) &\leq 2 \exp \left(-\frac{t^2\E{\overline Y} K }{3M}\right). \label{eq:chernoff}\end{aligned}$$
:::

Our main result is as follows.

::: theorem
[]{#thm:c_bound label="thm:c_bound"} Consider the above problem setup
(Section [1.4.1](#sec:analysis_setup){reference-type="ref"
reference="sec:analysis_setup"}), and assume that the expected value of
any table cell estimate during the TP procedure is lower bounded by
$B>0$. With probability at least $1-\delta$,
$$(1-\epsilon)d_i(x_i)\leq \hat d_i(x_i) \leq (1+\epsilon)d_i(x_i)$$ for
all $i$ and $x_i$, if the sample size used for all multiplication
operations is at least
$$K_{\min}(\epsilon, \delta) \in \bigO{\frac{C^2}{\epsilon^2}\frac{M}{B}\left(\log C + T + \log\frac{1}{\delta}\right)}.$$
:::

::: proof
*Proof.* Our proof is similar to the proof of Theorem
[\[thm:consistency\]](#thm:consistency){reference-type="ref"
reference="thm:consistency"}. The $\bigO{C^2 2^T}$ estimates from
Corollary [\[corr:nests\]](#corr:nests){reference-type="ref"
reference="corr:nests"} each satisfy the Chernoff bound with respect to
their expected value, and applying a union bound to these gives
$$\begin{aligned}
    P_{t}\left(\overline Y_j \notin \left[(1-t)\E{\overline Y_j}, (1+t)\E{\overline Y_j}\right]  \; \textrm{for any estimate $j$} \right) \leq \alpha_1 C^2 2^T \exp \left(-\frac{t^2 B K }{3M}\right) \label{eq:c_epsbound}\end{aligned}$$
where $\overline Y_j$ denotes the estimate made by the
[Multiply]{.smallcaps} procedure of some table cell $j$, and we
introduce constant $\alpha_1$ as above. Note that we have replaced
$\E{\overline Y}$ with the smaller $B$ on the RHS.

We claim that
$\overline Y_j \in \left[(1-t)\E{\overline Y_j}, (1+t)\E{\overline Y_j}\right]$
for all estimates $j$ implies
$\hat d_i(x_i) \in \left[(1-t)^{\alpha_2 C}d_i(x_i), (1+t)^{\alpha_2 C}d_i(x_i)\right]$
for all $i$ and $x_i$ and some constant $\alpha_2$. Similar to above, we
show this by assuming WLOG that for each estimate $\overline Y_j$,
$\overline Y_j = (1+t)\E{\overline Y_j}$, and examining how the error
propagates during the execution of our algorithm. The multiplicative
nature of the error makes this analysis substantially easier than the
additive case.

Specifically, suppose we have $F$ functions $\phi_1,\ldots,\phi_F$, and
wish to compute ${(1+t)\phi_1\cdots (1+t)\phi_F}$. The error factors
simply multiply together to give $(1+t)^F\phi_1\cdots \phi_F$.
Furthermore, suppose we wish to compute the sum
$(1+t)\phi_1 + \ldots +  (1+t)\phi_F$. The result is
$(1+t)(\phi_1 + \ldots + \phi_F)$ and the error factor $(1+t)$ does not
change. Thus, the error factor on the final $\hat d_i(x_i)$ depends only
on the number of [Multiply]{.smallcaps} calls made throughout the
algorithm relevant to the computation of $\hat d_i(x_i)$, which is no
greater than $\alpha_2 C$ for some $\alpha_2$. For each of these
$\alpha_2 C$ [Multiply]{.smallcaps} calls, we get an extra factor of
error $(1+t)$ on the marginal estimate, and hence the error factor of
$\hat d_i(x_i)$ for any $i$ and $x_i$ is bounded by $(1+t)^{\alpha_2 C}$
as required.

To complete the proof, we seek to bound the marginal error in the form
$\left[(1-\epsilon)d_i(x_i), (1+\epsilon)d_i(x_i)\right]$ for a chosen
$\epsilon$. We note that choosing
$t = \nicefrac{\log(1+\epsilon)}{\alpha_2 C}$ implies
$(1-t)^{\alpha_2 C} \geq 1-\epsilon$ and
$(1+t)^{\alpha_2 C} \leq (1+\epsilon)$: $$\begin{aligned}
    (1-t)^{\alpha_2 C} = \left(1-\frac{\log(1+\epsilon)}{\alpha_2 C}\right)^{\alpha_2 C} &\geq \left(1 - \frac{\epsilon}{\alpha_2 C}\right)^{\alpha_2 C} \geq 1 - \epsilon \label{eq:eps1}\\
    (1+t)^{\alpha_2 C} = \left(1+\frac{\log(1+\epsilon)}{\alpha_2 C}\right)^{\alpha_2 C} &\leq \exp\left(\log(1+\epsilon)\right) = 1+\epsilon. \label{eq:eps2}\end{aligned}$$
In ([\[eq:eps1\]](#eq:eps1){reference-type="ref" reference="eq:eps1"})
we use Bernoulli's inequality[^8] together with
$\log(1+\epsilon) \leq \epsilon$, and in
([\[eq:eps2\]](#eq:eps2){reference-type="ref" reference="eq:eps2"}) we
use $(1+x)^r \leq \exp(rx)$. Substituting this $t$ into
([\[eq:c_epsbound\]](#eq:c_epsbound){reference-type="ref"
reference="eq:c_epsbound"}), we obtain $$\begin{aligned}
    P_{t}\left(\hat d_i(x_i) \notin \left[(1-\epsilon)d_i(x_i), (1+\epsilon)d_i(x_i)\right]\right) \leq \alpha_1 C^2 2^T \exp \left(-\frac{BK\log^2(1+\epsilon)}{3M\alpha_2^2C^2}\right)\end{aligned}$$
Bounding the RHS of this expression by $\delta$ and rearranging gives
the required expression for $K_{\min}(\epsilon,\delta)$:
$$\begin{aligned}
    &\alpha_1 C^2 2^T \exp \left(-\frac{BK\log^2(1+\epsilon)}{3M\alpha_2^2C^2}\right) \leq \delta \\
    \implies &K \geq \frac{3M\alpha_2^2C^2}{B\log^2(1+\epsilon)}\log\left(\frac{\alpha_1C^22^T}{\delta}\right) =: K_{\min}(\epsilon,\delta).\end{aligned}$$
Noting that $\log(1+\epsilon) \geq \epsilon \log 2$ for
$0\leq \epsilon \leq 1$, we thus have
$$K_{\min}(\epsilon,\delta) \in \bigO{\frac{C^2}{\epsilon^2}\frac{M}{B}\left(\log C + T + \log\frac{1}{\delta}\right)}$$
as required. ◻
:::

Theorem [\[thm:c_bound\]](#thm:c_bound){reference-type="ref"
reference="thm:c_bound"} gives us (an asymptotic upper bound on) the
minimum number of samples required to ensure fixed relative error for
the unnormalised marginal estimates. The following Corollary extends
this to the normalised marginal estimates $\hat p_i(x_i)$.

::: corr
[]{#corr:c_bound label="corr:c_bound"} Under the same conditions as
Theorem [\[thm:c_bound\]](#thm:c_bound){reference-type="ref"
reference="thm:c_bound"}, the sample size required to limit the relative
error on the normalised marginal estimates $\hat p_i(x_i)$ to
$(1\pm\gamma)$ has the same asymptotic form, i.e.
$$K'_{\min}(\gamma, \delta) \in \bigO{\frac{C^2}{\gamma^2}\frac{M}{B}\left(\log C + T + \log\frac{1}{\delta}\right)}.$$
:::

::: proof
*Proof.* Suppose the unnormalised marginals have relative error bounded
by $(1\pm\epsilon)$, i.e.
$$(1-\epsilon)d_i(x_i)\leq \hat d_i(x_i) \leq (1+\epsilon)d_i(x_i)$$ for
all $i$ and $x_i$. Then since
$$\hat p_i(x_i) = \frac{\hat d_i(x_i)}{\sum_{x_i} \hat d_i(x_i)},$$ we
have
$$\hat p_i(x_i) < \frac{(1+\epsilon)d_i(x_i)}{\sum_{x_i} (1-\epsilon) d_i(x_i)} = \frac{(1+\epsilon)d_i(x_i)}{(1-\epsilon)\sum_{x_i} d_i(x_i)} = \frac{1+\epsilon}{1-\epsilon}p_i(x_i).$$
To bound the relative error on $\hat p_i(x_i)$ to $(1+\gamma)$, we set
$$\frac{1+\epsilon}{1-\epsilon} = 1+\gamma \implies \epsilon = \frac{\gamma}{\gamma+2}.$$
Since
$\frac{1}{\epsilon^2} = \left(\frac{\gamma+2}{\gamma}\right)^2 < \left(\frac{3}{\gamma}\right)^2 = \bigO{\frac{1}{\gamma^2}}$
for $\gamma < 1$, the increase in $K_{\min}$ required to bound the
normalised estimates rather than the unnormalised estimates is at most a
constant. Thus,
$$K'_{\min}(\gamma, \delta) \in \bigO{\frac{C^2}{\gamma^2}\frac{M}{B}\left(\log C + T + \log\frac{1}{\delta}\right)}.$$
as required. The case for the negative side of the bound is similar. ◻
:::

::: corr
[]{#corr:running_time label="corr:running_time"} To limit the relative
marginal error to $(1\pm\epsilon)$ with probability at least $1-\delta$
for fixed $\epsilon, \delta$, the running time
$\mathcal{T}_{\epsilon,\delta}$ of the TP algorithm is
$$\mathcal{T}_{\epsilon,\delta} \in \bigO{\frac{C^4}{\epsilon^2}\frac{M}{B}\left(\log C + T + \log\frac{1}{\delta}\right)}. \label{eq:tp_runtime}$$
:::

::: proof
*Proof.* The running time of the TP algorithm is bounded by
$\bigO{C^2K}$, since it is dominated by the $\bigO{C^2}$ approximate
multiplication steps. Therefore, this result is an immediate consequence
of Corollary [\[corr:c_bound\]](#corr:c_bound){reference-type="ref"
reference="corr:c_bound"}. ◻
:::

#### 5.4.3.1 Interpretation

We have shown that given some fixed relative error requirement for
marginal estimates, the number of samples we must use for
multiplications scales as ${C^2\frac{M}{B}(\log C + T)}$ and the running
time as ${C^4\frac{M}{B}(\log C + T)}$. This suggests some interesting
properties of the algorithm which we will now discuss. Firstly, as
previously stated, inference is in general $\mathcal{NP}$-hard and we
expect the running time for a fixed error requirement to be at least
exponential in the induced width $T$. Since an explicit exponential
dependence is not present in these bounds, we should expect the ratio
$M/B$ to be exponentially large for difficult problems. Recall that $M$
is an upper bound on all cells in all rank-one tensors used throughout
the algorithm, and $B$ is a lower bound on the estimation targets of all
sampling steps (that is, the expected values of all cells in all
sampling steps).[^9]

The interesting question is for what problems $M/B$ is low. Clearly
there do exist problem instances where tensor propagation will perform
well that have high treewidth and are therefore classically difficult.
As an extreme example of such a problem, consider a complete graph with
$T$ nodes and hence induced width $T$, whose single clique potential
function can be exactly represented with a sum of only two rank-one
tensors. By exploiting this tensor rank decomposition, the TP algorithm
can find (exact) marginals very quickly using
([\[eq:cp_marg\]](#eq:cp_marg){reference-type="ref"
reference="eq:cp_marg"}). The junction tree algorithm, however, will
take time exponential in $T$ to perform the marginalisation.[^10] One
may conjecture, therefore, that the ratio $M/B$ captures some measure of
how efficiently the potentials in the problem can be represented as
low-rank tensors. That is, in the worst case where the potentials do not
decompose well, $M/B$ is large and we recover the exponential dependence
on treewidth, but some cases admit decompositions where $M/B$ is small
and the algorithm performs well. We do not claim any precise results
relating to this intuition, however.

The second important point is that, whilst the parameters $C$, $B$ and
$T$ are fixed by the problem instance, $M$ depends on the particular
choice of tensor rank decomposition used and can be manipulated. This
suggests that we should choose rank decompositions whose cell values are
small.[^11] As well as having implications for the choice of initial
decompositions, this motivates the reweighting procedure we discuss in
the following section.

Finally, note that the above analysis is not valid if $B=0$, which
occurs if any of the initial clique potentials contain zeros. For this
reason, Theorem [\[thm:c_bound\]](#thm:c_bound){reference-type="ref"
reference="thm:c_bound"} cannot be used to establish consistency in the
general case.

## 5.5 Reweighting of decomposed functions

### 5.5.1 Max-norm reweighting {#sec:maxnorm}

Let $\phi$ be a potential function represented in decomposed form, i.e.

{% raw %}
$$
\begin{aligned}
    \phi_c(\mathbf{x}) &= \sum_k w_k\; \psi_{k}(\mathbf{x}).
%     &= \sum_{k=1}^{r} w_k^c\; \psi_{k,1}^c(x_1) \psi_{k,2}^c(x_2) \cdots \psi_{k,d}^c(x_d). \label{eq:fact}
\end{aligned}
$$
{% endraw %}

We are free to choose any new set of weights $\{w'_k\}$ that satisfy
$\sum_kw'_k=1$, by reweighting the $\psi_k$: 

$$\begin{aligned}
    \phi_c(\mathbf{x}) &= \sum_k w_k\; \psi_{k}(\mathbf{x}) \nonumber \\
    &= \sum_k w'_k \left(\frac{w_k}{w'_k} \psi_{k}(\mathbf{x})\right) \nonumber \\
    &= \sum_k w'_k \psi'_{k}(\mathbf{x}),\quad \psi'_k(\mathbf{x}) = \frac{w_k}{w'_k} \psi_{k}(\mathbf{x}).\label{eq:reweighting}\end{aligned}
$$

Note that this does not change the function $\phi_c$ represented by the
decomposition.[^12] In our case, we want to choose the $w'_k$ such that
the largest value $M$ across all rank-one tensors $\psi'_k$ is
minimised. This is achieved by scaling each $\psi'_k$ to have the same
maximum value. Specifically, we set $$\begin{aligned}
    \psi"_k(\mathbf{x}) &= \frac{\psi_k(\mathbf{x})}{\max_\mathbf{x}\psi_k(\mathbf{x})} \nonumber \\
    w"_k &= w_k \cdot {\max_\mathbf{x}\psi_k(\mathbf{x})} \label{eq:maxnorm}\end{aligned}$$
for all $k$, and then re-normalise so the weight sum is $1$, i.e.
$$\begin{aligned}
    w'_k &= \frac{w"_k}{\sum_l w"_l} \\
    \psi'_k(\mathbf{x}) &= \psi"_k \cdot \sum_l w"_l.\end{aligned}$$ The
resulting rank-one tensors $\psi'_k$ all share the same maximum value.
The new decomposition has maximum value $M$ that is optimal in the sense
that it is as small as possible for any similar reweighting. This is
true by the following argument: if the maximum value of all the
$\psi'_k$ were not equal, we could rescale the tensor(s) with lower
maxima to match the higher maximum. This would effectively lower $M$ for
this function after re-normalising the weights to sum to $1$.

Since the new decomposition has lower (or equal) maximum value $M$
compared to the original decomposition, applying this reweighting step
to every function prior to sampling will reduce (or maintain) the
overall $M$ for the whole algorithm. We therefore expect that using the
new decompositions for sampling will give better results. Note that the
new weights can be computed quickly, since we can express the maximum of
a rank-one tensor as the product of the maxima of each of its component
vectors, thereby avoiding iteration over the whole tensor. In our
experiments we refer to this reweighting procedure as *max-norm
reweighting*. Empirically, including this reweighting step in the TP
algorithm before every multiplication gives a noticable improvement in
performance (see Chapter
[\[cha:experiments\]](#cha:experiments){reference-type="ref"
reference="cha:experiments"}).

5.5.2 ### Minimum variance reweighting

As an alternative to reweighting terms in the decompositions to minimise
$M$, we can consider reweighting to optimise other objectives. A natural
question to consider is how the variance of the sampling steps in the
algorithm impacts the variance of the final marginal estimates.
Intuitively, if all the rank-one tensors in each decomposition are close
in magnitude to each other and the resulting sampling procedures have
low variance, we should expect the final marginals to exhibit low
variance.[^13]

In this section, we consider the problem of finding the reweighting
([\[eq:reweighting\]](#eq:reweighting){reference-type="ref"
reference="eq:reweighting"}) that minimises the sum of the variance of
the table cell estimates. Recall that given an iid sample of terms
$\sublist{\mathcal{K}}{K}$ drawn from the decomposition of $\phi$, the
estimator for a table cell $\phi(\mathbf{x})$ is
$$\hat\phi(\mathbf{x}) = \frac{1}{K}\sum_{i=1}^K \psi_{\mathcal{K}_i}(\mathbf{x}).$$
After reweighting, the new estimator is
$$\hat\phi'(\mathbf{x}) = \frac{1}{K}\sum_{i=1}^K \psi'_{\mathcal{K}'_i}(\mathbf{x}), \quad \psi'_k(\mathbf{x}) = \frac{w_k}{w'_k} \psi_{k}(\mathbf{x})$$
where $\sublist{\mathcal{K}'}{K}$ are instead drawn from the
decomposition of $\phi'$ (that is, according to weights $\{w'_k\}$
instead of $\{w_k\}$). Since the $\mathcal{K}'_i$ are iid, the variance
of the reweighted estimator is 

{% raw %}
$$\begin{aligned}
    \var{\hat\phi'(\mathbf{x})} &= \frac{1}{K}\var{\psi'_{\mathcal{K}'_i}(\mathbf{x})} \\
    &= \frac{1}{K} \left(\E{\psi'_{\mathcal{K}_i}(\mathbf{x})^2} - \E{\psi'_{\mathcal{K}_i}(\mathbf{x})}^2 \right)\\
    &= \frac{1}{K} \left(\sum_k w'_k \psi'_k(\mathbf{x})^2 - \phi(\mathbf{x})^2 \right) \\
    &= \frac{1}{K} \left(\sum_k w'_k \frac{w_k^2}{{w'}_k^2}\psi_k(\mathbf{x})^2 - \phi(\mathbf{x})^2 \right) \\
    &= \frac{1}{K} \left(\sum_k \frac{w_k^2}{{w'}_k}\psi_k(\mathbf{x})^2 - \phi(\mathbf{x})^2 \right).\end{aligned}
$$
{% endraw %}

We seek to minimise the sum of this variance over all variable
configurations. Noting that $K$ and the expected value
$\phi(\mathbf{x})$ are constant, the minimisation problem is
$$\begin{aligned}
    \{w'_k\}^* &= \argmin_{\{w'_k\}}\sum_\mathbf{x} \var{\hat\phi'(\mathbf{x})} \\
    &= \argmin_{\{w'_k\}}\sum_\mathbf{x}\sum_k \frac{1}{w'_k} (w_k\psi_k(\mathbf{x}))^2, \quad \textrm{such that} \; \sum_k w'_k=1.\end{aligned}$$
This can be solved with the method of Lagrange multipliers. The
Lagrangian is

{% raw %}
$$
\mathcal{L}(\{w'_k\}, \lambda) = \sum_\mathbf{x}\sum_k \frac{1}{w'_k} (w_k\psi_k(\mathbf{x}))^2 + \lambda\left(\sum_k w'_k-1\right).$$
Setting derivatives to zero yields $$\begin{aligned}
    \frac{\partial\mathcal{L}}{\partial w'_k} = -\sum_\mathbf{x}\frac{1}{{w'}^2_k}(w_k\psi_k(\mathbf{x}))^2 + \lambda = 0
    &\implies w'_k = \frac{w_k\sqrt{\sum_\mathbf{x}\psi_k(\mathbf{x})^2}}{\sqrt{\lambda}}. \nonumber \\
    \sum_k w'_k = 1 &\implies \frac{\sum_k w_k\sqrt{\sum_\mathbf{x}\psi_k(\mathbf{x})^2}}{\sqrt{\lambda}} = 1 \nonumber \\
    &\implies \sqrt{\lambda} = \sum_k w_k \sqrt{\sum_\mathbf{x}\psi_k(\mathbf{x})^2} \nonumber \\
    &\implies w'_k = \frac{w_k\sqrt{\sum_\mathbf{x}\psi_k(\mathbf{x})^2}}{\sum_l w_l\sqrt{\sum_\mathbf{x}\psi_l(\mathbf{x})^2}}, \label{eq:minvar}\end{aligned}
$$
{% endraw %}

giving us our new weights $\{w_k'\}$ as required. Note that the
denominator is simply performing normalisation so $\sum_kw'_k=1$.
Importantly, the numerator can be computed efficiently in the same
manner as the marginalisation in
([\[eq:cp_marg\]](#eq:cp_marg){reference-type="ref"
reference="eq:cp_marg"}), with the additional step of performing an
elementwise square operation on the $\psi_k$ tensor which is also cheap
due to its decomposition. Thus, the reweighting does not significantly
add to the running time.

In our experiments, we refer to this reweighting as the *minimum
variance reweighting*. Empirically, it yields about the same improvement
as the max-norm reweighting given above.

5.6 ## Related work {#sec:related}

In this section we give a brief overview of published literature that
relates to our work on the tensor propagation algorithm.

5.6.1 ### Message passing using structured approximations

The idea of using approximate representations of functions in
message-passing algorithms has been explored since the mid 1990s.
Notably, [@can00] describe "Penniless propagation", where they use
probability trees[^14] to represent potentials and messages in the
junction tree algorithm. The authors describe how to perform
multiplication and marginalisation operations by combining trees to
maintain reasonable approximations of potentials and messages while
limiting time and space usage. Early work using structured
representations also includes [@sta00], who use algebraic decision
diagrams (ADDs)[^15] in the context of solving Markov decision processes
(MDP), in an algorithm they call APRICODD.

Recently, [@gog13] formalised to this idea via a framework they call
"structured message passing", where potentials and messages are
represented in an abstract structured manner with the only requirement
that they define sum, product and division operators. The authors note
that if these operators are lossless we recover the belief propagation
algorithm, and show that if they are lossy but minimise KL divergence to
the true functions then we recover the well-known expectation
propagation (EP) algorithm. The authors present experiments using ADDs
and sparse hash tables as examples of structured representations.

Note that tensor rank decomposition is similar but more general than
ADDs for representing arbitrary tensors. An ADD partitions a tensor into
axis-aligned hyper-rectangles and it is possible to represent an ADD
that has $k$ hyper-rectangles using a tensor decomposition with $k$
terms, each representing a disjoint hyper-rectangle.[^16] The converse
is not true as the terms in a tensor decomposition may overlap. However,
the method of performing approximations in ADD and tensor decomposition
differs, and this may result in the methods having different strengths.
Sparse hash tables, on the other hand, can only handle cases with
extreme sparsity.

Other recent work includes [@sanner2012symbolic], who use piecewise
polynomial representations of potentials in "symbolic variable
elimination" with the goal of computing exact, closed-form solutions on
mixed discrete and continuous distributions that are non-Gaussian.

5.6.2 ### Particle-based approximations

In addition to exploring structured representations of functions, there
has been much interest in combining particle-based approximations with
message-passing algorithms as a means of approximate inference. Early
work includes [@daw94], where the authors describe "hybrid propagation",
a method of combining monte-carlo sampling with exact message passing on
different parts of the same graph, with the goal of allowing exact
methods to be used even when some variables are continuous. This is
further extended by [@kja95].

The concept of using sampling to approximate intermediate results in the
junction tree algorithm is described in a more general setting in
[@kol99]. This paper explains how one can maintain compact
representations of functions by combining sampling with statistical
density estimation techniques, and suggests a method of iteratively
updating parametric representations of potential functions and messages
based on samples. The authors note the potential advantages of this
approach for both discrete and continuous distributions. [@sud03] offer
a more concrete version of this idea called "Nonparametric belief
propagation" (NBP), which is based on loopy BP rather than junction
tree. The basic idea is to represent messages as mixtures of Gaussians,
and limit the number of terms in product operations by importance
sampling and fitting new mixtures to the resulting weighted particles.

Continuing this theme, [@ihl09] describe "particle belief propagation"
(PBP), a similar algorithm to NBP that avoids the need to fit Gaussian
kernels to sampled particles. This can be seen as a generalisation of
the popular particle filtering algorithm for Markov chains to general
graphs (though the authors analyse the algorithm for tree-structured
graphs only). More recent work in this area includes [@eli12; @lie15].

Our approach differs from these methods in that we sample entire
functions from the tensor rank decomposition, rather than individual
particles. This avoids the intermediate step of fitting kernel density
estimators as in NBP, and also also addresses the sparse nature of
sampled particles in PBP.

5.6.3 ### Tensor rank decomposition {#tensor-rank-decomposition}

Matrix factorisation, and more generally tensor rank decomposition, has
been applied to a variety of domains in machine learning and statistics
including dimensionality reduction, classification, object detection and
gene clustering [@pha11]. Notably, [@sav06] have investigated the tensor
rank decomposition in the context of probabilistic inference. The
authors show via a simple example that rank-decomposing the CPTs in a
Bayesian network is equivalent to augmenting the network with additional
variables, which can potentially lead to more efficient exact inference
via the junction tree algorithm. The authors also suggest the
possibility of using the decomposition for approximate message passing
in a similar manner as we have done in this thesis. To our knowledge,
however, this idea has not been further explored.

[^1]: In general, ([\[eq:cp_opt\]](#eq:cp_opt){reference-type="ref"
    reference="eq:cp_opt"}) may have no optimal solution, because there
    are cases termed *degenerate* where one can achieve arbitrarily
    small but nonzero error if $r$ is less than the true rank. This does
    not change the ALS algorithm.

[^2]: Formally, this means for any $\epsilon > 0$,
    $\lim_{K\to\infty} P(\vert\hat d_i(x_i) - d_i(x_i)\vert \geq \epsilon) = 0$
    for all $i$ and $x_i$.

[^3]: By 'table cells', we are referring to the individual entries in
    the multidimensional tables (tensors) that represent functions
    throughout the entire algorithm, including intermediate messages.

[^4]: Note that the expected value of a table cell estimate is not
    necessarily equal to the true value of that table cell in an
    imaginary parallel run of exact junction tree, because the expected
    value depends on previous sampling steps of the TP algorithm which
    may introduce error.

[^5]: The union bound, or *Boole's inequality*, states that for any set
    of events dependent or not, $\{A_i\}$,
    $P\left(\bigcup_i A_i\right) \leq \sum_i P(A_i)$.

[^6]: Note that we assume the 'new' error is introduced on the operands
    rather than the product; we could equivalently use
    $(\phi_1 + \beta t)(\phi_2 + t) + t$.

[^7]: Slutsky's theorem is a well-known result giving sufficient
    conditions for convergence of estimators. The result here is
    established by noting that the denominator is a sum of consistent
    estimators and is therefore consistent, and thus the ratio is
    consistent.

[^8]: $(1-x)^y \geq 1-xy$ for $0\leq x\leq 1,y\geq1$.

[^9]: We can construct an example where $M/B$ grows exponentially with
    the number of nodes $N$ in the graph as follows. Consider an
    length-$N$ chain with pairwise potentials
    $\phi(x, x') = \mathbbm{1}[x == x'] + 1$ and constant unary
    potentials $1$. The junction tree is also a chain. Messages from
    leaves to their neighbours have minimum value $3$, and messages to
    leaves have maximum value at least $2^{n-2}$. Thus,
    $M/B \geq 2^{n-2}/3$. This demonstrates that easy problems may also
    have high $M/B$. Thanks to Ye Nan for this example.

[^10]: We do not claim anything about $M/B$ in this problem but simply
    use it as an example to illustrate the ability of TP to make hard
    problems easier.

[^11]: Note that attempting to minimise $M$ by scaling potential
    functions by a constant does not improve the bound, since it equally
    affects $M$ and $B$ and cancels out.

[^12]: Reweighting the distribution to improve sampling behaviour whilst
    maintaining its expected value in this manner is the foundation of
    *importance sampling*, where we usually aim instead to minimise the
    sampling variance (discussed in the next section).

[^13]: A slightly more formal justification for this intuition is that
    if we draw a finite sample $\sublist{X}{K}$ from some distribution
    $p$, and then draw another sample $\sublist{X^*}{K}$ from the
    distribution represented by the finite first sample (a process known
    as 'bootstrapping'), then the resulting sample mean $\overline X^*$
    has approximately twice the variance of the sample mean of the first
    sampling step $\overline X$ (this can be shown via a straightforward
    application of the law of total variance). Since the final marginal
    estimates in TP are ultimately obtained via a similar bootstrapping
    process, we expect their variance can similarly be expressed in
    terms of the variance of the intermediate sampling steps, in some
    way.

[^14]: A probability tree is a directed labelled tree, where each
    internal node represents a variable and each leaf node represents a
    probability value [@can00]

[^15]: ADDs are a generalisation of probability trees that allow
    multiple parents and hence reuse of subtrees.

[^16]: Thanks to my supervisor, Prof Lee Wee Sun, for this observation.
