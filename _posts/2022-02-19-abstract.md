---
layout: post
title:  "Abstract"
main-title: "Abstract"
---

The ability to derive rational beliefs about the state of the world by
combining probabilistic models with observations, a process known as
*probabilistic inference*, is central to machine learning and
statistics. The central framework used for probabilistic inference is
that of *probabilistic graphical models* (PGMs). A PGM uses a graph to
represent a probability distribution. Inference is then performed by
running various algorithms over the graph. In general, exact inference
on large models is $\mathcal{NP}$-hard. In practice, PGMs are able to
achieve substantial speedup over brute force by exploiting structure in
the distribution they represent. Despite this, inference in many
real-world applications remains intractable.

In this thesis we propose a new algorithm for approximate inference in
discrete graphical models. Our algorithm is based on the well-known
*junction tree* message-passing algorithm for exact inference, but
approximates messages in a novel way by using a particular
representation for intermediate messages -- specifically, as sums of
rank-one tensors. We use this tensor rank decomposition to approximate
messages by sampling rank-one terms. This allows us to restrict the
tensor rank of intermediate messages and thus avoid the junction tree
algorithm's exponential time and space requirements.

We prove that our algorithm is asymptotically consistent, and give
sufficient conditions for it to perform well including when the
treewidth of the graph is large. We show experimentally that our
algorithm outperforms several popular competing methods on standard test
cases.
