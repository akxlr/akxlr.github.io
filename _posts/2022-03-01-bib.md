---
layout: post
title:  ""
main-title: "Bibliography"
---

AFSHAR, H. M., SANNER, S., AND ABBASNEJAD, E. 2015. Linear-time gibbs sam­pling in piecewise graphical models. In AAAI (2015), pp. 3461–3467. (p. 24) 

ANDRIEU, C., DE FREITAS, N., DOUCET, A., AND JORDAN, M. I. 2003. An intro­duction to MCMC for machine learning. *Machine learning* 50, 1-2, 5–43. (pp. 23, 24) 

BADER, B. W., KOLDA, T. G., ET AL. 2015. MATLAB tensor toolbox version 2.6. http://www.sandia.gov/˜tgkolda/TensorToolbox/. (p. 34) 

BESAG, J. 1974. Spatial interaction and the statistical analysis of lattice systems. *Journal of the Royal Statistical Society. Series B (Methodological)*, 192–236. (p. 9) 

BISHOP, C. 2006. *Pattern Recognition and Machine Learning*. Information Science and Statistics. Springer. (pp. 8, 11, 15, 23) 

CANO, A., MORAL, S., AND SALMERON, A. 2000. Penniless propagation in join trees. *International Journal of Intelligent Systems* 15, 11, 1027–1059. (p. 45)

DAWID, A. P., KJÆRULFF, U., AND LAURITZEN, S. L. 1994. Hybrid propagation in junction trees. In *International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems* (1994), pp. 85–97. Springer. (p. 46) 

DOMKE, J. 2014. Lecture slides: Undirected models (COMP4680/8650 2014, The Australian National University). (p. 8) 

DOMKE, J. 2015. Maximum likelihood learning with arbitrary treewidth via fast-mixing parameter sets. In *Advances in Neural Information Processing Systems* (2015), pp. 874–882. 

DOMKE, J. AND LIU, X. 2013. Projecting ising model parameters for fast mixing. In *Advances in Neural Information Processing Systems* (2013), pp. 665–673. (p. 25) 

ELIDAN, G., MCGRAW, I., AND KOLLER, D. 2012. Residual belief propaga­tion: Informed scheduling for asynchronous message passing. *arXiv preprint arXiv:1206.6837*. (pp. 21, 46)

GEMAN, S. AND GEMAN, D. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. *IEEE Transactions on pattern analysis and machine intelligence* 6, 721–741. (p. 24) 

GOGATE, V. AND DOMINGOS, P. 2013. Structured message passing. In *Uncertainty in Artifcial Intelligence (UAI)* (2013). (pp. 2, 45) 

GUENNEBAUD, G., JACOB, B., ET AL. 2010. Eigen v3. http://eigen.tuxfamily.org. (p. 50) 

HAZAN, T. AND SHASHUA, A. 2008. Convergent message-passing algorithms for inference over general graphs with convex free energies. (pp. 49, 51) 

HOEFFDING, W. 1963. Probability inequalities for sums of bounded random vari­ables. *Journal of the American statistical association* 58, 301, 13–30. (p. 34) 

IHLER, A. T. AND MCALLESTER, D. A. 2009. Particle belief propagation. In AIS­TATS (2009), pp. 256–263. (pp. 46, 59) 

ISING, E. 1925. Beitrag zur theorie des ferromagnetismus. *Zeitschrift fur¨ Physik A Hadrons and Nuclei* 31, 1, 253–258. (p. 10)

JORDAN, M. I. 2003. An introduction to probabilistic graphical models, chapter 2. http://people.eecs.berkeley.edu/˜jordan/prelims/chapter2.pdf. (p. 5) 

KJÆRULFF, U. 1995. HUGS: Combining exact inference and Gibbs sampling in junction trees. In *Proceedings of the Eleventh conference on Uncertainty in Artifcial In­telligence* (1995), pp. 368–375. Morgan Kaufmann Publishers Inc. (p. 46) 

KOLDA, T. G. AND BADER, B. W. 2009. Tensor decompositions and applications. SIAM *review* 51, 3, 455–500. (pp. 28, 29) 

KOLLER, D. AND FRIEDMAN, N. 2009. *Probabilistic Graphical Models: Principles and Techniques*. Adaptive computation and machine learning. MIT Press. (pp. 1, 2, 5, 8, 9, 15, 18, 19, 22, 23, 24) 

KOLLER, D., LERNER, U., AND ANGELOV, D. 1999. A general algorithm for ap­proximate inference and its application to hybrid bayes nets. In *Proceedings of the Fifteenth conference on Uncertainty in Artifcial Intelligence* (1999), pp. 324–333. Mor­gan Kaufmann Publishers Inc. (p. 46) 

LAFFERTY, J., MCCALLUM, A., AND PEREIRA, F. 2001. Conditional random felds: Probabilistic models for segmenting and labeling sequence data. In *Proceedings of the eighteenth international conference on machine learning*, ICML, Volume 1 (2001), pp. 282–289. 

LEE, D. D. AND SEUNG, H. S. 2001. Algorithms for non-negative matrix factoriza­tion. In *Advances in neural information processing systems* (2001), pp. 556–562. (p. 34) 

LENZ, W. 1920. Beitrag zum verst¨andnis der magnetischen erscheinungen in fes­ten k¨orpern. *Z. Phys*. 21, 613–615. (p. 10) 

LIENART, T., TEH, Y. W., AND DOUCET, A. 2015. Expectation particle belief propa­gation. In *Advances in Neural Information Processing Systems* (NIPS) (2015), pp. 3609– 3617. 

LIU, X. AND DOMKE, J. 2014. Projecting markov random feld parameters for fast mixing. In *Advances in Neural Information Processing Systems* (2014), pp. 1377–1385. (pp. 49, 50) 

LIU, Y. 2006. *Conditional graphical models for protein structure prediction*. PhD thesis, University of Pittsburgh. 

MOOIJ, J. M. 2010. libDAI: A free and open source C++ library for discrete ap­proximate inference in graphical models. *Journal of Machine Learning Research 11*, 2169–2173. (p. 50) 

MURPHY, K. P., WEISS, Y., AND JORDAN, M. I. 1999. Loopy belief propagation for approximate inference: An empirical study. In *Proceedings of the Fifteenth con­ference on Uncertainty in artifcial intelligence* (1999), pp. 467–475. Morgan Kaufmann Publishers Inc. (pp. 21, 22) 

PASKIN, M. A. 2003. A short course on graphical models. http://ai. stanford.edu/˜paskin/gm-short-course/. (p. 18) 

PHAN, A. H. AND CICHOCKI, A. 2011. PARAFAC algorithms for large-scale prob­lems. *Neurocomputing* 74, 11, 1970–1984. (p. 46)

PITMAN, E. J. G. 1936. Suffcient statistics and intrinsic accuracy. In *Mathemati­cal Proceedings of the Cambridge Philosophical Society*, Volume 32 (1936), pp. 567–579. Cambridge Univ Press. (p. 9) 

RON, D. 2008. *Property Testing: A Learning Theory Perspective*. Foundations and trends in machine learning. Now Publishers. (p. 38) 

SANNER, S. AND ABBASNEJAD, E. 2012. Symbolic variable elimination for discrete and continuous graphical models. In AAAI (2012). (p. 45) 

SAVICKY, P. AND VOMLEL, J. 2006. Tensor rank-one decomposition of probability tables. In *Proceedings of the Eleventh Conference on Information Processing and Man­agement of Uncertainty in Knowledge-based Systems (IPMU)* (2006), pp. 2292–2299. (p. 46) 

ST-AUBIN, R., HOEY, J., AND BOUTILIER, C. 2000. APRICODD: Approximate pol­icy construction using decision diagrams. In NIPS (2000), pp. 1089–1095. (p. 45) 

SUDDERTH, E. B., IHLER, A. T., ISARD, M., FREEMAN, W. T., AND WILLSKY, A. S. 2003. Nonparametric belief propagation. *Commun*. ACM 53, 95–103. (p. 46) 

VANNIEUWENHOVEN, N. 2015. *The tensor rank decomposition: Truncation and iden­tifability*. PhD thesis, Arenberg Doctoral School Faculty of Engineering Science. (pp. 28, 29) 

WAINWRIGHT, M. J., JAAKKOLA, T. S., AND WILLSKY, A. S. 2003. Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching. In AISTATS (2003). (p. 49) 

