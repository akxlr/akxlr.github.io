---
layout: post
title:  "Chapter 4"
main-title: "Approximate inference"
### Bibliography ###
lafferty2001conditional: "Lafferty et al. 2001"
liu2006conditional: "Liu 2006"
gog13: "Gogate and Domingos 2013"

jordan2003introduction: "Jordan 2003"
koller09: "Koller and Friedman 2009"
len20: "Lenz 1920"
isi25: "Ising 1925"
bis06: "Bishop 2006"
dom14: "Domke 2014"
pit36: "Pitman 1936"
besag1974spatial: "Besag 1974"
paskinslides: "Paskin 2003"
mur99: "Murphy et al. 1999"
eli12: "Elidan et al. 2012"
and03: "Andrieu et al. 2003"
gem84: "Geman and Geman 1984"
### end of Bibliography ###
---

For high-treewidth graphical models, exact inference using the
algorithms discussed in Chapter
[Chapter 3]({% link _posts/2022-02-22-chapter-3.md %})
is often intractable. In these cases, we can often obtain acceptable
results with approximate algorithms. This chapter gives a brief overview
of three popular approximate inference algorithms: loopy belief
propagation, mean-field variational inference and Gibbs sampling. Note
that a complete description of loopy BP and mean-field inference depends
on the complex mathematical foundations of *variational inference*, a
topic into which we to not delve deeply. Furthermore, the study of
approximate inference algorithms is an active area of research and as
such there is a vast number of related algorithms that we do not discuss
here. Rather, this chapter is intended as a brief overview of the
algorithms we use later in our later experiments. We discuss some other
algorithms more directly related to our contribution in Section
5.6.

## 4.1 Loopy belief propagation {#sec:loopy_bp}

The belief propagation algorithm for trees (Section 3.2) can be
modified in a straightforward way to return approximate marginals for
general (non-tree) graphs. Recall that the recursion
([3.1]({% link _posts/2022-02-22-chapter-3.md %}#eq:bp_msg))

$$\m{t}{s}(x_s) = \sum_{x_t} \phi_{t,s}(x_t,x_s) \phi_t(x_t) \prod_{u\neq s} \m{u}{t}(x_t) $$

is well-defined on trees, and we can calculate all messages
$\m{t}{s}(x_s)$ with two passes over the tree. The idea behind loopy
belief propagation is to simply perform the same message-passing
procedure on general graphs, iteratively updating messages until some
stopping criterion is satisfied. Specifically, we update messages
according to ([3.1]({% link _posts/2022-02-22-chapter-3.md %}#eq:bp_msg)) until convergence, or until a fixed iteration limit is reached.

Since leaf nodes may no longer be present, we must begin by initialising
messages and this is typically done by setting $\m{t}{s}(x_s)=1$ for all
messages. Alternatively messages can be initialised randomly; [{{page.mur99}}]
argues that this yields similar results. Message updates can be
performed in parallel, with a fixed sequence, with a random sequence, or
using heuristics such as "residual belief propagation" described by
[{{page.eli12}}]. For trees, loopy BP reduces to the familiar BP algorithm of
Section 3.2.

Loopy belief propagation lacks stringent theoretical guarantees. In
particular, the beliefs are not guaranteed to converge at all.
Empirically, however, when the beliefs do converge the results are often
quite good [{{page.mur99}}]. In the framework of variational inference, loopy BP
can be viewed as an attempt to obtain beliefs from some distribution
$q \approx p$ that solves 

$$
\begin{align}
    \min_q \textrm{KL}(q \| p) \tag{4.1}\label{eq:min_kl}
\end{align}
$$

where $$\textrm{KL}(\cdot \| \cdot)$$ is the Kullback-Leibler divergence, a
common information-theoretic measure of distance between probability
distributions defined as

$$\textrm{KL}(q \| p) = \sum_{\mathbf{x}} q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p(\mathbf{x})}.$$

Briefly, computing the objective in (\ref{eq:min_kl}) involves an intractable quantity known as the
*Gibbs free energy* $F_\textrm{Gibbs}(P,Q)$, which we can approximate
with the tractable *Bethe free energy* $F_\textrm{Bethe}(P,Q)$ based on
the observation that for tree-structured distributions
$F_\textrm{Gibbs} = F_\textrm{Bethe}$. We then consider the problem of
minimising $F_\textrm{Bethe}$, and by writing the Lagrangian dual of
this problem, we arrive at a set of fixed-point updates that are exactly
the messages ([3.1]({% link _posts/2022-02-22-chapter-3.md %}#eq:bp_msg))
[{{page.mur99}}; {{page.koller09}}]. Loopy BP is therefore
mathematically principled, but this does not imply any particular
guarantee of accuracy.

[{{page.koller09}}] discusses some additional heuristics to improve performance
of loopy BP, including methods of improving the rate of convergence and
avoiding local minima.

## 4.2 Mean-field variational inference

Mean-field variational inference also considers the problem of solving
(\ref{eq:min_kl}), but instead restricts $q$ to a class of
tractable distributions, namely those that fully factorise. That is, we
attempt to solve 

$$
\begin{align}
    q_{\textrm{MF}} = \min_{q\in\textrm{FACT}} \textrm{KL}(q \| p) \quad \textrm{where} \quad \textrm{FACT} = \left\{ q \;\middle|\; q(\sublist{x}{\mathcal{D}}) = \prod_{i=1}^\mathcal{D} q_i(x_i) \right\}. \tag{4.2}\label{eq:mf}
\end{align}
$$

We can then perform inference efficiently using $q_\textrm{MF}$ instead
of $p$.[^1] Applying the method of Lagrange multipliers to (\ref{eq:mf}), we obtain
an iterative coordinate ascent algorithm 

{% raw %}
$$
\begin{align}
q_i(x_i) = \frac{1}{Z_i}\exp\left(\sum_{\phi : X_i \in \scope{(\phi)}} \mathbb{E}{_{\mathbf{X_j}\sim q}}[{\log \phi(x_i,\mathbf{x}_j)]} \right). \tag{4.3}\label{eq:mf2}
\end{align}
$$
{% endraw %}

Here, $q_i(x_i)$ for each variable $X_i$ is expressed in terms of its
neighbours in the graph, and we cycle through nodes $X_i$ in turn
performing this update until convergence. In each step we fix $q_j(x_j)$
for all other variables $x_j$ and compute a new estimate for $q_i(x_i)$
based on the current estimates of the $q_j$. The expectation 
$\mathbb{E}{_{\mathbf{X_j}\sim q}}[{\log \phi(x_i,\mathbf{x}_j)]}$ 
 is the (log of the) geometric average of a single potential $\phi$ involving the target
variable $X_i$, under the assumption that the other variables in
$\scope{(\phi)}$ are distributed according to the current $q_j(x_j)$. We
add the averages for each $\phi$ that contains $X_i$ in its scope,
exponentiate, and then normalise. This gives a new $q_i(x_i)$ and we
repeat this process until convergence. For a derivation of
(\ref{eq:mf2}), see [{{page.koller09}}].

Unlike loopy BP, mean-field is guaranteed to converge, but there are two
issues. Firstly, this convergence is to a local optimum only and
secondly the class FACT considerably restricts $q$ and it is unlikely
that $p\in\textrm{FACT}$ even if we could optimise globally. *Structured
mean-field* attempts to address the second problem by considering the
more general case where $q$ belongs to the family

$$q(\sublist{x}{\mathcal{D}}) = \frac{1}{Z}\prod_j\phi_j(\mathbf{x_j})$$

which can provide greater flexibility at the cost of increased
computational complexity [{{page.koller09}}].

## 4.3 Sampling

The third major approach to approximate inference we discuss is
sampling. In general, any inference query can be answered given a set of
$K$ independent samples $$\{\mathbf{x}^i\}_{i=1}^K$$ from $p$ by
considering the sampling distribution of the variables of interest.
Marginals are obtained as 

$$
\begin{aligned}
    P(X_j=y) \approx \frac{1}{K} \sum_{i=1}^K \mathbb{1}[x^i_j == y]
\end{aligned}
$$

where $\mathbb{1}[\cdot]$ is the indicator function.[^2] Drawing a set
of independent samples from an arbitrary distribution, however, is
difficult. There are various methods for sampling from low-dimensional
distributions or distributions with certain parametric forms (for
example, Gaussian distributions) [{{page.bis06}}]. In general however, for
arbitrary high-dimensional distributions, the most effective method
known is *Markov-chain Monte Carlo* (MCMC) [{{page.koller09}}; {{page.and03}}].

### 4.3.1 Markov-chain Monte Carlo

MCMC involves creating a Markov chain whose stationary distribution[^3]
is equal to the target distribution $p$, and generating successive
sample points by applying the chain transition function. Each sample
point is dependent on the previous sample point, but in many cases, if
the chain is long enough, distant sample points are almost independent
of each other and we can extract an approximately random sample from $p$
[{{page.and03}}].

The main MCMC algorithms currently in use are Metropolis-Hastings (MH),
and variants which can be seen as special cases of MH including Gibbs
sampling and slice sampling. These methods differ in how they chooose
the next sample in the chain given the current sample. In this thesis,
we make use of Gibbs sampling.

#### 4.3.1.1 Gibbs sampling

The basic sequential Gibbs sampling procedure [{{page.gem84}}] is as follows:

-   Choose some initial set of variable configurations $\mathbf{x}^0$.

-   For each variable $X_i$ in turn, draw a new value from
    $p(x_i \given \mathbf{x}^0_{-i})$. That is, draw a new value from
    the conditional distribution given the current setting for all other
    variables.

-   After a single pass over all variables, record the resulting state
    as a new sample point $\mathbf{x}^1$, and repeat to obtain the
    desired number of samples.

Under mild conditions, this Markov chain has stationary distribution
equal to the desired distribution $p$ [{{page.and03}}; {{page.koller09}}], and therefore
with enough steps we will achieve a random sample from $p$. In a typical
implementation, there is a *burn-in* phase where the first $M$ samples
are ignored for some $M$, and we usually take only every $N^\textrm{th}$
sample for some thinning parameter $N$ to improve the independence of
sample points. In our experiments we also use *random restarts*, i.e.
reset the chain state to a random configuration every $R$ iterations for
some $R$. The goal of this is to improve mixing, which we discuss below.

Gibbs sampling is attractive in discrete graphical models because the
conditional distributions from which we must sample,
$p(x_i \given \mathbf{x}^0_{-i})$, depend only on a subset of variables
(called the *Markov blanket* of $X_i$), which often makes the sampling
step straightforward [@and03]. For MRFs, the Markov blanket is simply
the set of neighbours of $X_i$ in the graph.[^4] There are various
extensions to Gibbs sampling including *blocked Gibbs sampling* where
groups of variables are sampled together, and *collapsed Gibbs sampling*
where one or more of the conditioning variables are integrated out
before sampling from the conditional distributions. In certain
situations these can improve the rate of convergence to the stationary
distribution. Recent work includes [{{page.afshar2015linear}}], who describe
efficient Gibbs sampling algorithms for piecewise graphical models.

#### 4.3.1.2 Mixing time {#sec:mixing}

The success of MCMC depends on how closely the Markov chain of samples
resembles a true random sample from $p$. If the Markov chain converges
to its stationary distribution quickly (conceptually if it 'explores'
the state space properly), the resulting sample is a good approximation.
For some distributions, however, sample points are so tightly dependent
on one another that the chain gets stuck in local high-probability
states and can take exponential time to converge. This property is
formalised by the concept of *mixing time*. The mixing time of a Markov
chain is defined to be the number of steps $t$ required for the chain to
produce samples whose distribution is guaranteed to be $\epsilon$-close
to its limiting stationary distribution $p$ from any starting state.[^5]
We say a distribution is *slow-mixing* if its mixing time is high and
*fast-mixing* otherwise. Inference in slow-mixing distributions is often
difficult precisely because obtaining an independent sample via MCMC is
hard. For various reasons, other algorithms also tend to perform worse
on slow-mixing distributions.

In Ising models and general pairwise Markov random fields, used in our
experiments, mixing time is closely related to form of the pairwise
potential functions. To illustrate this, recall that Ising models have
pairwise potentials $\phi_{ij}(x_i,x_j) = \exp (w_{ij}x_i x_j)$. If
parameters $w_{ij}$ are all large and positive, the model strongly
prefers neighbouring variables to be equal and assigns low probability
if they are opposite. The distribution therefore has two distinct modes,
one where all variables are $1$ and the other where all are $-1$. If a
run of Gibbs sampling reaches one of these modes, it will be difficult
to escape. The conditional distribution of any individual variable will
strongly favour maintaining the same sign, so successive Gibbs
iterations always choose not to swap variables and never (in reasonable
time) explore the other mode, even though it may have equal probability.
Large, positive $w_{ij}$ therefore give slow-mixing distributions. In
general though, precise results relating mixing time to properties of
potential functions are not known. Recent work has investigated
projecting MRF parameters onto sets known to be fast-mixing as a form of
approximate inference [@domke13; @domke14; @domke15].

[^1]: Note that ([\[eq:mf\]](#eq:mf){reference-type="ref"
    reference="eq:mf"}) does not imply that the $q_i$ are marginals of
    $q$; rather we are optimising for low KL divergence of the entire
    product to the true distribution.

[^2]: $1$ if $\cdot$ is true and $0$ otherwise.

[^3]: The limiting distribution of states of the Markov chain after
    running a large number of transitions from any starting point.

[^4]: Also, for some models, the conditional distributions can be
    obtained analytically.

[^5]: In this definition, distance is measured by the *total variation
    distance*
    $\|p - q\|_\textrm{TV} = \frac{1}{2} \sum_{\mathbf{x}} |p(\mathbf{x}) - q(\mathbf{x})|$.
