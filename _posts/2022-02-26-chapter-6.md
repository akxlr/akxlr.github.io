# Experiments {#cha:experiments}

To evaluate the performance of tensor propagation in practice, we have
tested the algorithm against popular existing approximate inference
methods using both Ising models and random MRFs as test cases. Ising
models are a standard benchmark for approximate inference algorithms,
and we use random graphs to give additional insights into when our
approach performs well, inspired by [@domke14].

## Experimental setup

Our experimental setup broadly follows that of [@domke14]. For each test
graph, algorithm performance was assessed based on errors in estimated
marginals, computed by comparing approximations to exact marginals.
Exact marginals were obtained using the junction tree algorithm;
experiments were chosen to be small enough for exact inference to be
feasible. We use the mean absolute difference over all nodes
$$\frac{1}{\mathcal{D}} \sum_{i=1}^\mathcal{D} |P_\textrm{exact}(X_i=1) - P_\textrm{approx}(X_i=1)|$$
as the error measure. Unless otherwise indicated, all results shown are
averages over 100 trials and each trial uses a separate, random model
instance with the specified parameters. Error bars indicate standard
error of the mean.

We test our algorithm against four popular existing approximate
inference methods: loopy belief propagation (labelled BP), mean-field
(MF), tree-reweighted BP (TRW) and Gibbs sampling. These are discussed
in Chapter [\[cha:approximate\]](#cha:approximate){reference-type="ref"
reference="cha:approximate"}, with the exception of tree-reweighted BP
which is described in [@wai03]. These algorithms are commonly used as a
benchmark for evaluating new methods of approximate inference
[@haz08; @domke13; @domke14]. The following parameters were chosen
heuristically based on empirical performance:

-   Loopy BP: Update schedule sequential using a random sequence;
    maximum $10^4$ iterations; tolerance for convergence $10^{-12}$

-   Mean-field: Maximum $10^4$ iterations; tolerance for convergence
    $10^{-12}$

-   Tree-reweighted BP: Sequential updates using a random sequence; tree
    sample size of $10^4$ used to set weights; tolerance for convergence
    $10^{-12}$

-   Gibbs: Burn-in 100 passes; restart chain with random initialisation
    every 1000 passes; record one sample per pass (pass = cycle once
    over all variables); running time limited as indicated in text.

The accuracy of Gibbs sampling improves with increased running time, so
for fair comparisons each run of tensor propagation was timed and the
subsequent Gibbs sampling run was limited to the same wall-clock time.
The other algorithms do not benefit from increased running time in this
manner; they were simply run until convergence using the parameters
indicated above.[^1]

All core numerical code used C++11, with Python 2.7 used to generate
graphs and plot results. For all inference algorithms other than tensor
propagation, we use existing implementations from the libDAI package
[@moo10]. This is commonly used in published literature as a benchmark
implementation of these methods. We implemented tensor propagation using
the Eigen library [@eigen] for matrix computations and making use of
code from the libDAI junction tree algorithm. For the clique
decompositions, we use the first solution of the symmetric rank-2
decomposition
([\[eq:symm_decomp\]](#eq:symm_decomp){reference-type="ref"
reference="eq:symm_decomp"}) in all cases. Except for the plots showing
comparison of reweighting methods, we use the max-norm reweighting
method (Section [\[sec:maxnorm\]](#sec:maxnorm){reference-type="ref"
reference="sec:maxnorm"}) as this gave the best performance empirically.
To simplify the interpretation of results, we fix the parameter $H$
(multiplication size above which we use approximate multiplication) to
$1$ unless otherwise noted, i.e. all nontrivial multiplications use the
sampling approximation. All experiments were executed using a single
core of an Intel i5 1.4GHz processor.

## Ising models

Following [@domke14], random Ising models are constructed with unary
potentials chosen uniformly from $[-1,1]$ and pairwise potentials chosen
uniformly from $[0,d_e]$ (attractive interactions) or $[-d_e, d_e]$
(mixed interactions), for some interaction strength parameter $d_e$. We
include separate results for mixed and attractive interactions, because
these models have fundamentally different characteristics that cause the
various approximate inference algorithms to perform differently. Unless
otherwise stated, all models are $10\times 10$ grids and $d_e$ is set to
$2.0$.

### Performance vs sample size $K$

\

Figure
[\[fig:ising_samplesize\]](#fig:ising_samplesize){reference-type="ref"
reference="fig:ising_samplesize"} shows marginal error for five sample
sizes $K\in\{10, 10^2, 10^3, 10^4, 10^5\}$ for tensor propagation with
no reweighting, max-norm reweighting and minimum variance reweighting,
alongside results for existing approximate inference methods. As we
increase the sample size used in approximate multiplications, the
algorithm performs better as expected (lower marginal error).
Intuitively, the lower error is a result of more accurate approximate
multiplications due to lower sampling variance. Obviously, this
increased accuracy comes at the cost of longer running time and higher
memory usage.

On all models, loopy BP, mean-field and tree-reweighted BP perform
poorly. This matches previously published results for Ising models
[@haz08; @domke13; @domke14]. In fairness, these methods converge
comparatively quickly and do not benefit from the extra running time
given to TP and Gibbs. On mixed interaction models, as expected, Gibbs
sampling performs well. Tensor propagation is only able to match the
performace of Gibbs when the sample size is large. However, after about
$10^4$ samples (equivalent to 10 seconds running time on our test
machine), the performance of reweighted tensor propagation overtakes
Gibbs (even though Gibbs is also allowed increased running time to match
TP). On attractive interaction models, Gibbs performs poorly due to slow
mixing whereas the performance of tensor propagation is only slightly
worse than the fast-mixing case. Because of this, tensor propagation
outperforms all other algorithms by a significant margin beyond $10^3$
samples (1 second).

Reweighting with either scheme yields a noticeable improvement versus no
reweighting. Reweighting effectively reduces the sampling variance and
thus should be expected to have a similar effect to increasing sample
size, but without significantly increasing running time. We observe no
noticeable difference between the two reweighting schemes. The
similarity of the performance of these schemes is not surprising given
the similar objectives
([\[eq:maxnorm\]](#eq:maxnorm){reference-type="ref"
reference="eq:maxnorm"}) and
([\[eq:minvar\]](#eq:minvar){reference-type="ref"
reference="eq:minvar"}). Finally, we observe that the standard error for
all algorithms is larger on attractive models, reflecting the increased
difficulty of these models.

### Performance vs problem difficulty

To test how well our method scales with the problem difficulty, we run
two experiments. First, we fix the interaction strength at $d_e=2.0$ and
vary the number of variables in the Ising grid. Inference is harder on
larger models due to the increased treewidth so we expect all methods to
perform worse as grid size increases. Second, we fix the grid size and
vary the interaction strength. As previously discussed, stronger
interactions lead to slower mixing and hence inference is generally more
difficult. We therefore expect stronger interactions to lead to worse
performance. In each plot, we show separate lines for TP with $K=100$
and $K=100000$ samples.

\

Figure [\[fig:ising_size\]](#fig:ising_size){reference-type="ref"
reference="fig:ising_size"} shows results for square $N\times N$ Ising
models with widths $N\in\{3,\ldots,16\}$ (that is, between $9$ and $256$
total variables).[^2] As expected, all models perform worse (higher
marginal error) as the problem size increases. For TP, this reflects the
larger number of approximate multiplication steps in the algorithm and
hence higher variance of final marginal estimates. For attractive
models, TP with $10^5$ samples again outperforms all other methods by a
significant margin. For mixed interactions, performance is about the
same as Gibbs sampling and we cannot draw a clear conclusion as to which
performs better. Again, we notice that the performance of TP does not
differ substantially between models with mixed vs attractive potentials.

\

Figure [\[fig:ising_intstr\]](#fig:ising_intstr){reference-type="ref"
reference="fig:ising_intstr"} shows the effect of increasing interaction
strength in the Ising models. Again, with sample size $K=10^5$, TP
outperforms the other algorithms on all problem sizes. All models
perform worse as interaction strength increases, including TP,
reflecting the increased difficulty discussed above. For TP, the
relationship between interaction strength and error is likely mediated
by the choice of initial decomposition. In our case, the decomposition
([\[eq:symm_decomp\]](#eq:symm_decomp){reference-type="ref"
reference="eq:symm_decomp"}) leads to a higher ratio of $M/B$ in
([\[eq:tp_runtime\]](#eq:tp_runtime){reference-type="ref"
reference="eq:tp_runtime"}) if the interaction strength $\theta$ is
large and this may explain the decrease in performance. It is likely
that any choice of decomposition will suffer similar problems.

### Distribution of estimated marginals

In this section we examine the distribution of marginal estimates over a
large number of runs, rather than just the mean marginal error. We do
this by running the algorithm $500$ times on a single $10\times 10$
Ising instance. For each node, we group the $500$ marginal estimates
$P(X_i=1)$ into bins, and plot a histogram of bin frequencies.

\
\

First, we observe that larger $K$ leads to more accurate and
concentrated marginal estimates as predicted by the consistency property
of TP -- in other words, as $K$ increases, the resulting distribution of
estimates converges to the true marginal for each node.[^3] For low $K$,
as expected, the distribution of marginal estimates exhibits high
variance (and hence, large expected error). In this case, we observe
that the algorithm generally favours predictions that are close to zero
or one, but does not always choose between these two extremes correctly.
This is similar in effect to Gibbs sampling on slow-mixing models, which
gets 'stuck' on a particular configuration and hence gives extreme
marginal estimates that are often incorrect. For high $K$, the marginal
estimates are roughly normally distributed about the true values.

### Effect of sampling threshold $H$

Finally, we show the effects of the sampling threshold $H$ on the
algorithm. Recall that $H$ controls the number of terms above which we
approximate the product with Algorithm
[\[alg:multiply\]](#alg:multiply){reference-type="ref"
reference="alg:multiply"} versus computing the product exactly, and was
set to $1$ in all of the above experiments. We expect higher $H$ to lead
to more accurate results, at the expense of increased running time.

As expected, figure
[\[fig:ising_threshold\]](#fig:ising_threshold){reference-type="ref"
reference="fig:ising_threshold"} demonstrates the beneficial effect of
performing more exact multiplications on marginal error. $H$ does not
have a substantial effect for larger sample sizes in the ranges that we
can reasonably test; we therefore omit results for $K>100$. Note,
however, that the running time increases rapidly, as shown in figure
[\[fig:ising_threshold_time\]](#fig:ising_threshold_time){reference-type="ref"
reference="fig:ising_threshold_time"}. Our results suggest that this
increase in running time is generally not justified, since the
improvement is relatively small compared to the increase in running
time.

\

\

## Random graphs

Our second set of test cases are random MRF instances. Out of the
complete set of possible edges, each edge $(i,j)$ is independently
present with probability $0.5$. Potentials are pairwise and of the same
form as Ising potentials, i.e.
$\phi_{ij}(x_i,x_j) = \exp (w_{ij}x_i x_j)$. As with Ising models, each
node also has a univariate potential $\phi_i(x_i) = \exp (b_i x_i)$.
Pairwise strengths $w_{ij}$ are chosen uniformly with $d_e=2.0$ and
$b_i$ uniformly from $[-1,1]$ as above. Compared to Ising models, these
graphs have significantly higher edge density which leads to larger
clusters in the junction tree relative to the model size. The behaviour
of each algorithm is therefore potentially quite different to that of
Ising models.

### Performance vs sample size $K$

\

In figure
[\[fig:rand_samplesize\]](#fig:rand_samplesize){reference-type="ref"
reference="fig:rand_samplesize"}, we again observe TP outperforming the
other algorithms for high sample sizes. These differences are more
pronounced than for Ising models (except in the attractive case where
TRW performs surprisingly well). For mixed interactions, Gibbs sampling
appears not to be as effective as on Ising models. TP runs extremely
fast on these graphs, likely because the clusters in the junction tree
are large and therefore the number of message-passing steps is low (and
the complexity of computing messages in TP does not depend significantly
on the size of the table it is approximating, but rather the number of
samples $K$). Thus, Gibbs is not allowed much time to run and therefore
performs badly. Combined with the aforementioned mixing issues for
attractive graphs, this leads to degenerate identical samples and hence
equal error for the first three sample sizes in this plot.

### Performance vs problem difficulty

\

Figure [\[fig:rand_size\]](#fig:rand_size){reference-type="ref"
reference="fig:rand_size"} shows the effect of increasing graph size on
the marginal error for each algorithm. As with Ising models, we expect
larger graphs to be more difficult and hence lead to higher marginal
error. This is observed, and for mixed interactions we note that TP
scales at about the same rate as Gibbs sampling with graph size, similar
to Ising models. We notice, however, that the magnitude of the marginal
error for TP on large mixed graphs is significantly larger than on Ising
models. This is not surprising given the density of these graphs (and
therefore their high treewidth), and suggests that TP is not immune to
bad performance in fundamentally hard problems. In particular, this
supports our conjecture that the ratio $M/B$ hides this dependence on
treewidth for such problems.

Interestingly, for large, dense graphs with attractive interactions, TP
performs extremely well and, somewhat counterintuitively, significantly
better than for mixed interactions. This provides further evidence that
the performance of TP is not strongly dependent on treewidth in all
cases. The difference is likely due to the different properties of the
tensor decomposition between mixed and attractive interactions, and this
example would therefore be interesting to investigate further.

## Running time

For completeness, Figure
[\[fig:times\]](#fig:times){reference-type="ref" reference="fig:times"}
shows the wall-clock time required for the algorithm at various sample
sizes on our test machine, and hence the running time allowed for Gibbs
sampling. Note the linear dependence on sample size $K$, and also the
faster running time for random graphs, both of which match our analysis.

\

## Summary

Our tests demonstrate convincing performance gains of tensor propagation
over Gibbs sampling and the other algorithms tested. These gains were
generally more evident on slow-mixing models, where other methods
performed poorly, suggesting that these may be good candidates for TP.
On fast-mixing models, we were able to match the performance of Gibbs in
most cases and outperform it in some cases. Given a higher sample size
$K$, TP results improve with a predicted linear increase in running
time. Our algorithm appears to scale well with problem size and
difficulty, giving accurate marginals for all Ising model sizes tested
in reasonable time. On random graphs, TP also outperforms other methods
but does not necessarily give accurate marginals when the graph size is
large and the pairwise interactions are mixed-sign. For large,
attractive-only random graphs, however, TP performs very well. We
demonstrate consistency of the algorithm experimentally, showing that as
$K$ increases, the distribution of marginal estimates converges on the
true marginals.

Note that whilst the methods we compared against are commonly used to
benchmark approximate inference methods, the literature on approximate
inference methods is vast, and we cannot claim that TP outperforms all
of these algorithms. An interesting next step would be to test TP
against the recent sampling approaches described in Section
[\[sec:related\]](#sec:related){reference-type="ref"
reference="sec:related"}, such as particle belief propagation [@ihl09].

[^1]: Note that because we are comparing tensor propagation with Gibbs
    based on matched running time, the comparisons that follow are
    potentially implementation dependent. Since libDAI is a widely used
    library in published PGM research, and is written in C++, it is fair
    to assume its implementations are efficient and therefore any bias
    will be against tensor propagation. We have made an effort, however,
    to keep our implementation of tensor propagation fast.

[^2]: Beyond width $16$, the memory requirements of exact junction tree
    mean we can no longer obtain exact marginals on our test machine.

[^3]: Experimentally, increasing $K$ further does indeed lead to the
    predicted collapse of the distribution of marginals to a single
    point; we omit plots for higher $K$ since the concentrated
    histograms are not very informative.
